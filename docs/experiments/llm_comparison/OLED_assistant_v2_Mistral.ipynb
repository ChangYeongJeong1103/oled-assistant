{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî¨ AI-Driven OLED Assistant (Domain-Specific RAG)\n",
    "\n",
    "## üéØ Project Overview\n",
    "\n",
    "**Goal:** Build an intelligent AI assistant for OLED display engineering and technical support.\n",
    "\n",
    "**Key Features:**\n",
    "- üõ°Ô∏è Strict RAG: Answers based on documents, and rejects if relevant documents are not available\n",
    "- üéØ Relevance Score: Similarity score + sigmoid transformation-based 3-tier decision system (RAG / NO_ANSWER / OFF_TOPIC)\n",
    "- üìà Quantitative Evaluation: Quantitative evaluation of Strict RAG answer quality using LLM-as-a-judge (Specificity / Relevance / Factuality)\n",
    "- üíº Engineering Support: Answers questions about OLED processes, device properties, and optical simulation\n",
    "\n",
    "---\n",
    "## üõ†Ô∏è Required Packages\n",
    "\n",
    "### üì¶ Core Dependencies\n",
    "- **langchain**: LangChain framework (RAG pipeline construction)\n",
    "- **langchain-community**: PDF/DOCX loaders and integration features\n",
    "- **langchain-openai**: Chat API wrapper for OpenAI and OpenAI-compatible servers (e.g., Ollama/vLLM)\n",
    "- **docarray**: Python vector search engine (lightweight and fast)\n",
    "- **pypdf**: PDF text extraction\n",
    "- **docx2txt**: DOCX text extraction\n",
    "- **tiktoken**: Token calculation (for cost estimation)\n",
    "- **python-dotenv**: Environment variable management (.env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1\n",
    "# %pip install --upgrade pip\n",
    "# %pip install -U langchain langchain-community langchain-openai\n",
    "# %pip install -U docarray pypdf docx2txt tiktoken openai python-dotenv\n",
    "# %pip install -U sentence-transformers  # üîë For HuggingFace embeddings (sentence-transformers/all-MiniLM-L6-v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîê Environment Variables and API Key Configuration\n",
    "\n",
    "**What This Section Does:**\n",
    "- Loads environment variables from the `.env` file in the project directory\n",
    "- By default looks for `OPENAI_API_KEY`, but **this notebook's Strict RAG experiment uses local Mistral-Nemo (ollama_local)**, so it's not required\n",
    "- Loads the key in advance for future evaluation with OpenAI-based LLMs (e.g., gpt-4o-mini)\n",
    "- The `python-dotenv` package automatically reads the `.env` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load OpenAI API Key from .env file\n",
    "# This cell loads environment variables (especially the API key) before any API calls\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load all variables from .env file into environment\n",
    "# The .env file should be in the same directory as this notebook\n",
    "load_dotenv()\n",
    "\n",
    "# Check if API key was successfully loaded\n",
    "if os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    print(\"‚úÖ OpenAI API key loaded successfully from .env file\")\n",
    "else:\n",
    "    print(\"‚ùå Warning: OPENAI_API_KEY not found in .env file\")\n",
    "    print(\"   Please create a .env file with: OPENAI_API_KEY=your-key-here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Configuration (Hyperparameters)\n",
    "\n",
    "These are the main hyperparameters you can adjust to customize the OLED Assistant system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: User Configuration (Hyperparameters)\n",
    "# =======================================\n",
    "# üî¨ Hyperparameter Tuning Strategy (Strict RAG):\n",
    "# =======================================\n",
    "# Step 1: RAG Answer Quality Optimization\n",
    "#   - Tuning parameters: CHUNK_SIZE, CHUNK_OVERLAP, TOP_K_DOCUMENTS\n",
    "#\n",
    "# Step 2: Strict RAG Threshold Settings\n",
    "#   - Tuning parameters: RELEVANCE_THRESHOLD\n",
    "#   - Goal: Boldly reject if content is not in documents\n",
    "\n",
    "DOCS_FOLDER = \"../data\"  # Folder where OLED technical documents are stored\n",
    "DB_PATH = \"../chroma_db\" # Vector DB storage path (Persistence)\n",
    "# üîÅ LLM Configuration: Currently using local Mistral-Nemo 12B (Ollama)\n",
    "LLM_MODEL = \"mistral-nemo\"      # Must match ollama model name\n",
    "LLM_PROVIDER = \"llama_local\"    # OpenAI ‚Üí Local OpenAI-compatible server\n",
    "# ========================================\n",
    "# üîß STEP 1: RAG Answer Quality Settings\n",
    "# ========================================\n",
    "# üìê CHUNK_SIZE: Chunk size (character count)\n",
    "#    - Smaller: Precise search, but risk of context fragmentation\n",
    "#    - Larger: Context preservation, but risk of topic mixing\n",
    "CHUNK_SIZE = 3000  \n",
    "\n",
    "# üìê CHUNK_OVERLAP: Overlap between chunks (character count)\n",
    "#    - Ensures context connection, prevents information loss at boundaries\n",
    "CHUNK_OVERLAP = 500  \n",
    "\n",
    "# üìê TOP_K_DOCUMENTS: Number of chunks to retrieve\n",
    "#    - Smaller: Faster, only accurate top results\n",
    "#    - Larger: Diverse perspectives, but noise increases\n",
    "TOP_K_DOCUMENTS = 4 \n",
    "\n",
    "# üìê LLM_TEMPERATURE: Answer creativity\n",
    "#    - 0.0: Deterministic, fact-based (suitable for Strict RAG)\n",
    "#    - 0.7+: Creative, diverse expressions\n",
    "LLM_TEMPERATURE = 0.2  \n",
    "\n",
    "# ========================================\n",
    "# ‚öôÔ∏è STEP 2: Strict RAG Settings\n",
    "# ========================================\n",
    "# üìê SIGMOID_MIDPOINT: Center point of score distribution\n",
    "#    - Higher: Lower scores become even lower (stricter)\n",
    "#    - Needs adjustment after testing with new embedding model\n",
    "SIGMOID_MIDPOINT = 0.50  \n",
    "\n",
    "# üìê SIGMOID_STEEPNESS: Score separation intensity\n",
    "#    - Higher: Middle scores pushed to extremes\n",
    "SIGMOID_STEEPNESS = 18  \n",
    "\n",
    "# üìê RELEVANCE_THRESHOLD: Answerability threshold\n",
    "#    - Returns 'No Answer' if below this score\n",
    "#    - Needs adjustment after testing with new embedding model\n",
    "RELEVANCE_THRESHOLD = 0.6\n",
    "\n",
    "# 6Ô∏è‚É£ Test Question List\n",
    "# Various questions to test Relevance Score distribution\n",
    "# Expected distribution: High(0.8+) ‚Üí Medium(0.5-0.7) ‚Üí Low(0.2-0.4) ‚Üí Very Low(<0.2)\n",
    "TEST_QUERIES = [\n",
    "    # Case 1: High relevance, must-be-RAG\n",
    "    \"What are the key degradation mechanisms of a blue phosphorescent OLED?\",\n",
    "    \n",
    "    # Case 2: High but slightly lower than Case 1\n",
    "    \"How does exciton diffusion length affect charge separation efficiency in organic semiconductor devices?\",\n",
    "    \n",
    "    # Case 3: Display but non-OLED\n",
    "    \"What are the major challenges in mass transfer processes for MicroLED displays?\",  \n",
    "    \n",
    "    # Case 4: Non-display semiconductor device\n",
    "    \"How does doping concentration affect the electron mobility in silicon MOSFETs?\",\n",
    "    \n",
    "    # Case 5: Science but non-electronics\n",
    "    \"Describe the physical principles used to reduce aerodynamic drag in automotive design\",\n",
    "\n",
    "    # Case 6: Non-science, off-topic\n",
    "    \"Recommend a good hiking trail near Santa Clara for a weekend trip\",  \n",
    "]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"‚úÖ Configuration Complete!\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"üìÇ Documents Folder: {DOCS_FOLDER}\")\n",
    "print(f\"üíæ DB Path: {DB_PATH}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Logging, Monitoring & Error Handling Setup\n",
    "\n",
    "**Why add this before hyperparameter tuning?**\n",
    "\n",
    "When experimenting with various hyperparameters, the following are needed:\n",
    "1. üìù **Logging**: Record what's happening (for debugging)\n",
    "2. ‚è±Ô∏è **Monitoring**: Track performance (response time, cost)\n",
    "3. üõ°Ô∏è **Error Handling**: Handle errors gracefully (API failures, timeouts)\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ What We Will Track\n",
    "\n",
    "**Performance Metrics:**\n",
    "- ‚è±Ô∏è Response time per query\n",
    "- üí∞ Token usage and estimated cost\n",
    "- üìä Mode distribution (RAG vs NO_ANSWER vs OFF_TOPIC)\n",
    "\n",
    "**Error Handling:**\n",
    "- ‚ö†Ô∏è LLM API errors (Rate limit, timeout from OpenAI or local Mistral and other OpenAI-compatible servers)\n",
    "- üìÅ Document loading errors (file not found, corrupted PDF)\n",
    "- üîÑ Automatic retry on transient errors\n",
    "\n",
    "**Experiment Tracking:**\n",
    "- üíæ Automatically save results to CSV file\n",
    "- üìà Compare various hyperparameter settings\n",
    "- üéØ Find optimal settings based on data\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Logging, Monitoring & Error Handling Setup\n",
    "# This cell sets up logging, cost tracking, error handling, and experiment tracking\n",
    "\n",
    "import logging\n",
    "import time\n",
    "import csv\n",
    "import os\n",
    "from datetime import datetime\n",
    "from functools import wraps\n",
    "import tiktoken\n",
    "\n",
    "# ========================================\n",
    "# üìù LOGGING SETUP\n",
    "# ========================================\n",
    "# Create logs directory if it doesn't exist\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "\n",
    "# Configure logging\n",
    "# Note: remove existing handlers before reconfiguring so the cell can be run multiple times safely\n",
    "# Remove all existing handlers so repeated cell runs behave reliably\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "    handler.close()  # Clean up handler resources\n",
    "\n",
    "# Then apply the base configuration\n",
    "log_filename = f'logs/AI_OLED_assistant_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log'\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_filename),\n",
    "        logging.StreamHandler()  # Also print to console\n",
    "    ],\n",
    "    force=True  # Python 3.8+: force reconfiguration to avoid duplicate handlers\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.info(f\"üìù Logging initialized: {log_filename}\")\n",
    "\n",
    "# ========================================\n",
    "# üí∞ COST TRACKING\n",
    "# ========================================\n",
    "class CostTracker:\n",
    "    \"\"\"\n",
    "    Track token usage and estimated API costs.\n",
    "    \n",
    "    OpenAI pricing (as of 2024):\n",
    "    - GPT-4o-mini: $0.150 per 1M input tokens, $0.600 per 1M output tokens\n",
    "    \"\"\"\n",
    "    \n",
    "    # Pricing per 1M tokens (USD)\n",
    "    PRICING = {\n",
    "        \"gpt-5\": {\"input\": 1.25, \"output\": 10.00},\n",
    "        \"gpt-5-mini\": {\"input\": 0.25, \"output\": 2.00},\n",
    "        \"gpt-4o-mini\": {\"input\": 0.15, \"output\": 0.60},\n",
    "        # Local LLMs (Mistral, etc.) have cost set to 0, only monitor tokens\n",
    "        \"mistral-nemo\": {\"input\": 0.0, \"output\": 0.0},\n",
    "    }\n",
    "    \n",
    "    def __init__(self, model_name=\"gpt-4o-mini\"):\n",
    "        \"\"\"Initialize cost tracker for a specific model.\"\"\"\n",
    "        self.model_name = model_name\n",
    "        # Use safe default encoding in case tiktoken doesn't know the model name\n",
    "        try:\n",
    "            self.encoding = tiktoken.encoding_for_model(model_name)\n",
    "        except Exception:\n",
    "            self.encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        self.total_input_tokens = 0\n",
    "        self.total_output_tokens = 0\n",
    "    \n",
    "    def count_tokens(self, text):\n",
    "        \"\"\"Count tokens in a text string.\"\"\"\n",
    "        return len(self.encoding.encode(text))\n",
    "    \n",
    "    def add_tokens(self, input_text, output_text):\n",
    "        \"\"\"\n",
    "        Add token counts for input and output.\n",
    "        \n",
    "        Args:\n",
    "            input_text: Input prompt text\n",
    "            output_text: Model response text\n",
    "        \"\"\"\n",
    "        input_tokens = self.count_tokens(input_text)\n",
    "        output_tokens = self.count_tokens(output_text)\n",
    "        \n",
    "        self.total_input_tokens += input_tokens\n",
    "        self.total_output_tokens += output_tokens\n",
    "        \n",
    "        return input_tokens, output_tokens\n",
    "    \n",
    "    def get_cost(self):\n",
    "        \"\"\"Calculate total cost in USD.\"\"\"\n",
    "        if self.model_name not in self.PRICING:\n",
    "            return 0.0  # Unknown model\n",
    "        \n",
    "        pricing = self.PRICING[self.model_name]\n",
    "        input_cost = (self.total_input_tokens / 1_000_000) * pricing[\"input\"]\n",
    "        output_cost = (self.total_output_tokens / 1_000_000) * pricing[\"output\"]\n",
    "        \n",
    "        return input_cost + output_cost\n",
    "    \n",
    "    def get_summary(self):\n",
    "        \"\"\"Get summary of token usage and cost.\"\"\"\n",
    "        return {\n",
    "            \"input_tokens\": self.total_input_tokens,\n",
    "            \"output_tokens\": self.total_output_tokens,\n",
    "            \"total_tokens\": self.total_input_tokens + self.total_output_tokens,\n",
    "            \"estimated_cost_usd\": self.get_cost()\n",
    "        }\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset token counters.\"\"\"\n",
    "        self.total_input_tokens = 0\n",
    "        self.total_output_tokens = 0\n",
    "\n",
    "# Initialize global cost tracker\n",
    "cost_tracker = CostTracker(model_name=LLM_MODEL)\n",
    "\n",
    "# ========================================\n",
    "# üõ°Ô∏è ERROR HANDLING UTILITIES\n",
    "# ========================================\n",
    "def retry_on_api_error(max_retries=2, delay=2):\n",
    "    \"\"\"\n",
    "    Decorator to retry function on API errors.\n",
    "    \n",
    "    Args:\n",
    "        max_retries: Maximum number of retry attempts\n",
    "        delay: Delay in seconds between retries\n",
    "    \n",
    "    Usage:\n",
    "        @retry_on_api_error(max_retries=2, delay=2)\n",
    "        def api_call():\n",
    "            # Your API call here\n",
    "            pass\n",
    "    \"\"\"\n",
    "    def decorator(func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            last_exception = None\n",
    "            \n",
    "            for attempt in range(max_retries + 1):\n",
    "                try:\n",
    "                    return func(*args, **kwargs)\n",
    "                except Exception as e:\n",
    "                    last_exception = e\n",
    "                    error_type = type(e).__name__\n",
    "                    \n",
    "                    # Log the error\n",
    "                    if attempt < max_retries:\n",
    "                        logger.warning(f\"‚ö†Ô∏è {error_type} on attempt {attempt + 1}/{max_retries + 1}: {str(e)}\")\n",
    "                        logger.info(f\"üîÑ Retrying in {delay} seconds...\")\n",
    "                        time.sleep(delay)\n",
    "                    else:\n",
    "                        logger.error(f\"‚ùå Failed after {max_retries + 1} attempts: {str(e)}\")\n",
    "            \n",
    "            # If all retries failed, raise the last exception\n",
    "            raise last_exception\n",
    "        \n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "def safe_file_load(file_path, loader_class):\n",
    "    \"\"\"\n",
    "    Safely load a file with error handling.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to file\n",
    "        loader_class: LangChain loader class (PyPDFLoader or Docx2txtLoader)\n",
    "    \n",
    "    Returns:\n",
    "        List of loaded documents, or empty list if failed\n",
    "    \"\"\"\n",
    "    try:\n",
    "        loader = loader_class(file_path)\n",
    "        documents = loader.load()\n",
    "        logger.info(f\"‚úÖ Loaded: {os.path.basename(file_path)} ({len(documents)} pages/sections)\")\n",
    "        return documents\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"‚ùå File not found: {file_path}\")\n",
    "        return []\n",
    "    except PermissionError:\n",
    "        logger.error(f\"‚ùå Permission denied: {file_path}\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Error loading {os.path.basename(file_path)}: {type(e).__name__} - {str(e)}\")\n",
    "        return []\n",
    "\n",
    "# ========================================\n",
    "# üìä EXPERIMENT TRACKING\n",
    "# ========================================\n",
    "class ExperimentTracker:\n",
    "    \"\"\"Track and save hyperparameter experiment results.\"\"\"\n",
    "    \n",
    "    def __init__(self, csv_filename=\"hyperparameter_experiments.csv\"):\n",
    "        \"\"\"Initialize experiment tracker.\"\"\"\n",
    "        self.csv_filename = csv_filename\n",
    "        self.current_experiment = {}\n",
    "        \n",
    "        # Create CSV file with headers if it doesn't exist\n",
    "        if not os.path.exists(csv_filename):\n",
    "            with open(csv_filename, 'w', newline='') as f:\n",
    "                # CSV header for Strict RAG (no LLM baseline comparison)\n",
    "                writer = csv.DictWriter(f, fieldnames=[\n",
    "                    'timestamp',  # 1\n",
    "\n",
    "                    # RAG Quality (Step 1)\n",
    "                    'chunk_size', 'chunk_overlap', 'top_k', 'temperature',  # 2-5\n",
    "\n",
    "                    # System Architecture (Step 2) - Only what's used in Strict RAG\n",
    "                    'relevance_threshold', 'sigmoid_midpoint', 'sigmoid_steepness',  # 6-8\n",
    "\n",
    "                    # Performance\n",
    "                    'avg_response_time_sec', 'total_tokens', 'estimated_cost_usd',  # 9-11\n",
    "\n",
    "                    # Mode Distribution (Strict RAG: RAG, NO_ANSWER_IN_DOCS, OFF_TOPIC only)\n",
    "                    'mode_rag_count', 'mode_no_answer_count', 'mode_off_topic_count', 'mode_error_count',  # 12-15\n",
    "\n",
    "                    # Score Metrics (RAG only, no LLM baseline)\n",
    "                    'avg_relevance_score', 'avg_rag_score',  # 16-17\n",
    "                    'avg_rag_specificity', 'avg_rag_relevance', 'avg_rag_factuality',  # 18-20\n",
    "\n",
    "                    # Relevance separation metrics (Q1‚ÄìQ2 vs Q5‚ÄìQ6)\n",
    "                    'rel_mean_pos', 'rel_mean_neg', 'rel_gap', 'rel_margin',  # 21-24\n",
    "\n",
    "                    'notes'  # 25\n",
    "                ])\n",
    "                writer.writeheader()\n",
    "    \n",
    "    def start_experiment(self, config):\n",
    "        \"\"\"\n",
    "        Start tracking a new experiment.\n",
    "        \n",
    "        Args:\n",
    "            config: Dictionary with hyperparameter settings\n",
    "        \"\"\"\n",
    "        # Strict RAG modes: RAG, NO_ANSWER_IN_DOCS, OFF_TOPIC only\n",
    "        self.current_experiment = {\n",
    "            'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            **config,\n",
    "            'response_times': [],\n",
    "            'mode_counts': {\n",
    "                'RAG': 0, \n",
    "                'NO_ANSWER_IN_DOCS': 0,\n",
    "                'OFF_TOPIC': 0,\n",
    "                'ERROR': 0\n",
    "            }\n",
    "        }\n",
    "        logger.info(f\"üß™ Starting experiment with config: {config}\")\n",
    "    \n",
    "    def log_query(self, response_time, mode):\n",
    "        \"\"\"Log a single query result.\"\"\"\n",
    "        self.current_experiment['response_times'].append(response_time)\n",
    "        self.current_experiment['mode_counts'][mode] += 1\n",
    "    \n",
    "    def save_experiment(self, \n",
    "                       avg_relevance_score=None, avg_rag_score=None,\n",
    "                       avg_rag_specificity=None, avg_rag_relevance=None, avg_rag_factuality=None,\n",
    "                       rel_mean_pos=None, rel_mean_neg=None, rel_gap=None, rel_margin=None,\n",
    "                       notes=\"\"):\n",
    "        \"\"\"Save Strict RAG experiment results to CSV.\n",
    "\n",
    "        Args\n",
    "        ----\n",
    "        avg_relevance_score : float | None\n",
    "            Average relevance score (0-1)\n",
    "        avg_rag_score : float | None\n",
    "            Average RAG answer quality (1-10)\n",
    "        avg_rag_specificity : float | None\n",
    "            Average RAG specificity score (1-10)\n",
    "        avg_rag_relevance : float | None\n",
    "            Average RAG relevance score (1-10)\n",
    "        avg_rag_factuality : float | None\n",
    "            Average RAG factuality score (1-10)\n",
    "        rel_mean_pos : float | None\n",
    "            Q1‚ÄìQ2 (OLED core) relevance average\n",
    "        rel_mean_neg : float | None\n",
    "            Q5‚ÄìQ6 (OFF-TOPIC) relevance average\n",
    "        rel_gap : float | None\n",
    "            rel_mean_pos - rel_mean_neg\n",
    "        rel_margin : float | None\n",
    "            min(Q1‚ÄìQ2) - max(Q5‚ÄìQ6)\n",
    "        notes : str\n",
    "            Additional notes about the experiment\n",
    "        \"\"\"\n",
    "        cost_summary = cost_tracker.get_summary()\n",
    "        \n",
    "        # Calculate averages\n",
    "        avg_response_time = sum(self.current_experiment['response_times']) / len(self.current_experiment['response_times']) if self.current_experiment['response_times'] else 0\n",
    "        \n",
    "        # Row data for Strict RAG (matches CSV header order)\n",
    "        row = {\n",
    "            'timestamp': self.current_experiment['timestamp'],\n",
    "\n",
    "            # RAG Quality (Step 1)\n",
    "            'chunk_size': self.current_experiment.get('chunk_size'),\n",
    "            'chunk_overlap': self.current_experiment.get('chunk_overlap'),\n",
    "            'top_k': self.current_experiment.get('top_k'),\n",
    "            'temperature': self.current_experiment.get('temperature'),\n",
    "\n",
    "            # System Architecture (Step 2) - Only what's used in Strict RAG\n",
    "            'relevance_threshold': self.current_experiment.get('relevance_threshold'),\n",
    "            'sigmoid_midpoint': self.current_experiment.get('sigmoid_midpoint'),\n",
    "            'sigmoid_steepness': self.current_experiment.get('sigmoid_steepness'),\n",
    "            \n",
    "            # Performance\n",
    "            'avg_response_time_sec': f\"{avg_response_time:.2f}\",\n",
    "            'total_tokens': cost_summary['total_tokens'],\n",
    "            'estimated_cost_usd': f\"${cost_summary['estimated_cost_usd']:.4f}\",\n",
    "\n",
    "            # Mode Distribution (Strict RAG: RAG, NO_ANSWER_IN_DOCS, OFF_TOPIC only)\n",
    "            'mode_rag_count': self.current_experiment['mode_counts']['RAG'],\n",
    "            'mode_no_answer_count': self.current_experiment['mode_counts']['NO_ANSWER_IN_DOCS'],\n",
    "            'mode_off_topic_count': self.current_experiment['mode_counts']['OFF_TOPIC'],\n",
    "            'mode_error_count': self.current_experiment['mode_counts']['ERROR'],\n",
    "            \n",
    "            # Score Metrics (RAG only, no LLM baseline)\n",
    "            'avg_relevance_score': f\"{avg_relevance_score:.3f}\" if avg_relevance_score is not None else \"N/A\",\n",
    "            'avg_rag_score': f\"{avg_rag_score:.2f}\" if avg_rag_score is not None else \"N/A\",\n",
    "            'avg_rag_specificity': f\"{avg_rag_specificity:.2f}\" if avg_rag_specificity is not None else \"N/A\",\n",
    "            'avg_rag_relevance': f\"{avg_rag_relevance:.2f}\" if avg_rag_relevance is not None else \"N/A\",\n",
    "            'avg_rag_factuality': f\"{avg_rag_factuality:.2f}\" if avg_rag_factuality is not None else \"N/A\",\n",
    "\n",
    "            # Relevance separation metrics (Q1‚ÄìQ2 vs Q5‚ÄìQ6)\n",
    "            'rel_mean_pos': f\"{rel_mean_pos:.3f}\" if rel_mean_pos is not None else \"N/A\",\n",
    "            'rel_mean_neg': f\"{rel_mean_neg:.3f}\" if rel_mean_neg is not None else \"N/A\",\n",
    "            'rel_gap': f\"{rel_gap:.3f}\" if rel_gap is not None else \"N/A\",\n",
    "            'rel_margin': f\"{rel_margin:.3f}\" if rel_margin is not None else \"N/A\",\n",
    "\n",
    "            'notes': notes\n",
    "        }\n",
    "        \n",
    "        # Append to CSV\n",
    "        with open(self.csv_filename, 'a', newline='') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=row.keys())\n",
    "            writer.writerow(row)\n",
    "        \n",
    "        logger.info(f\"üíæ Experiment saved to {self.csv_filename}\")\n",
    "        logger.info(f\"üìä Summary: Avg time={avg_response_time:.2f}s, Cost=${cost_summary['estimated_cost_usd']:.4f}\")\n",
    "\n",
    "# Initialize global experiment tracker\n",
    "experiment_tracker = ExperimentTracker()\n",
    "\n",
    "print(\"‚úÖ Logging, Monitoring & Error Handling configured successfully!\")\n",
    "print(f\"üìù Logs will be saved to: logs/\")\n",
    "print(f\"üìä Experiment results will be saved to: hyperparameter_experiments.csv\")\n",
    "print(f\"üí∞ Cost tracking enabled for model: {LLM_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìñ How to Use Logging, Monitoring & Error Handling\n",
    "\n",
    "**‚úÖ Automatic Logging:**\n",
    "- All operations are logged to `logs/AI_OLED_assistant_[timestamp].log`.\n",
    "- Logs include timestamps, error messages, and performance metrics.\n",
    "- Monitoring runs automatically in the background.\n",
    "\n",
    "**‚úÖ Cost Tracking:**\n",
    "- Token usage for each query is automatically calculated.\n",
    "- Estimated API cost is calculated in real time.\n",
    "- A summary is printed after each experiment.\n",
    "\n",
    "**‚úÖ Error Handling:**\n",
    "- API failures automatically retry (up to 2 times).\n",
    "- File loading errors are caught and written to the logs.\n",
    "- User-friendly error messages are returned.\n",
    "\n",
    "**‚úÖ Experiment Tracking:**\n",
    "- All hyperparameter experiments are saved to `hyperparameter_experiments.csv`.\n",
    "- Includes: response time, token usage, cost, mode distribution, and more.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÅ Document Loading Setup\n",
    "\n",
    "**What This Section Does:**\n",
    "This is a configuration section for document loading. The actual loading happens in the next section.\n",
    "\n",
    "**Environment:**\n",
    "- Designed for local execution\n",
    "- Reads OLED technical documents directly from the `data/` folder\n",
    "\n",
    "## üìÑ Document Loading and Chunking\n",
    "\n",
    "**What This Section Does:**\n",
    "This is where OLED technical documents are actually loaded and processed.\n",
    "\n",
    "**Step-by-step Process:**\n",
    "\n",
    "1. **File Discovery** (`glob.glob` with `recursive=True`)\n",
    "   - Scans for PDF/DOCX files in the `data/` folder and **all subfolders**\n",
    "   - Automatically recognizes documents even if organized in folders\n",
    "\n",
    "2. **Document Loading** (`PyPDFLoader`, `Docx2txtLoader`)\n",
    "   - PDF ‚Üí text conversion (one page = one document)\n",
    "   - DOCX ‚Üí text conversion (one section = one document)\n",
    "   - `safe_file_load()` wrapper protects against crashes from corrupted files\n",
    "\n",
    "3. **Text Chunking** (`RecursiveCharacterTextSplitter`)\n",
    "   - Splits long documents into smaller chunks\n",
    "   - Each chunk is `CHUNK_SIZE` characters (e.g., current setting: 3000)\n",
    "   - `CHUNK_OVERLAP` characters of overlap between chunks (e.g., current setting: 500)\n",
    "   - Why overlap? Prevents context loss when sentences are split at chunk boundaries\n",
    "\n",
    "**Why We Chunk Documents:**\n",
    "- **LLM Token Limits**: Models have a maximum input size (e.g., 128K tokens)\n",
    "- **Better Retrieval**: Smaller chunks match queries more accurately\n",
    "- **Context Preservation**: Overlap prevents important information loss at boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Load and Chunk Documents\n",
    "# This cell loads all PDF and DOCX files from the docs folder and splits them into chunks\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# ========================================\n",
    "# 0. Fast path: If ChromaDB already exists, skip loading & chunking\n",
    "#    - If DB exists, skip PDF loading + chunking, and only load existing DB in Cell 12\n",
    "# ========================================\n",
    "if os.path.exists(DB_PATH) and os.listdir(DB_PATH):\n",
    "    print(f\"üìÇ Existing ChromaDB found at {DB_PATH}\")\n",
    "    print(\"‚è≠Ô∏è Skipping document loading & chunking (using existing DB only).\")\n",
    "    logger.info(f\"Skip loading/chunking because DB already exists at {DB_PATH}\")\n",
    "\n",
    "else:\n",
    "    # ========================================\n",
    "    # STEP 1: Find all PDF and DOCX files\n",
    "    # ========================================\n",
    "    logger.info(f\"üìÇ Scanning folder: {DOCS_FOLDER}\")\n",
    "\n",
    "    # Check if docs folder exists before proceeding\n",
    "    if not os.path.exists(DOCS_FOLDER):\n",
    "        logger.error(f\"‚ùå Docs folder not found: {DOCS_FOLDER}\")\n",
    "        raise FileNotFoundError(f\"Documents folder '{DOCS_FOLDER}' does not exist\")\n",
    "\n",
    "    # Use glob to find all PDF and DOCX files in the folder (including subfolders)\n",
    "    # Use recursive=True and ** to search subfolders as well\n",
    "    pdf_files = glob.glob(os.path.join(DOCS_FOLDER, \"**/*.pdf\"), recursive=True)\n",
    "    docx_files = glob.glob(os.path.join(DOCS_FOLDER, \"**/*.docx\"), recursive=True)\n",
    "\n",
    "    print(f\"üìÇ Found {len(pdf_files)} PDF files and {len(docx_files)} DOCX files in {DOCS_FOLDER}/ (including subfolders)\")\n",
    "    logger.info(f\"Found {len(pdf_files)} PDFs and {len(docx_files)} DOCX files\")\n",
    "\n",
    "    # Check if any files found - warn if folder is empty\n",
    "    if len(pdf_files) == 0 and len(docx_files) == 0:\n",
    "        logger.warning(f\"‚ö†Ô∏è No PDF or DOCX files found in {DOCS_FOLDER}/\")\n",
    "        print(f\"‚ö†Ô∏è WARNING: No documents found. Please add PDF or DOCX files to {DOCS_FOLDER}/\")\n",
    "\n",
    "    # ========================================\n",
    "    # STEP 2: Load all documents into memory\n",
    "    # ========================================\n",
    "    all_documents = []  # This will store all loaded document pages/sections\n",
    "\n",
    "    # Load PDF files using PyPDFLoader (one page = one document)\n",
    "    # safe_file_load() wraps the loader with error handling (won't crash on corrupt files)\n",
    "    print(f\"\\nüìÑ Loading PDF files...\")\n",
    "    for pdf_file in pdf_files:\n",
    "        # safe_file_load() handles errors gracefully - returns [] on failure\n",
    "        documents = safe_file_load(pdf_file, PyPDFLoader)\n",
    "        all_documents.extend(documents)  # Add all pages from this PDF to our collection\n",
    "\n",
    "    # Load DOCX files using Docx2txtLoader (one section = one document)\n",
    "    print(f\"\\nüìÑ Loading DOCX files...\")\n",
    "    for docx_file in docx_files:\n",
    "        documents = safe_file_load(docx_file, Docx2txtLoader)\n",
    "        all_documents.extend(documents)\n",
    "\n",
    "    # ========================================\n",
    "    # STEP 3: Validate that documents were loaded\n",
    "    # ========================================\n",
    "    if len(all_documents) == 0:\n",
    "        logger.error(\"‚ùå No documents were successfully loaded\")\n",
    "        raise ValueError(\"Failed to load any documents. Please check file formats and permissions.\")\n",
    "\n",
    "    print(f\"\\n‚úÖ Total documents loaded: {len(all_documents)}\")\n",
    "    logger.info(f\"Successfully loaded {len(all_documents)} document sections\")\n",
    "\n",
    "    # ========================================\n",
    "    # STEP 4: Split documents into chunks\n",
    "    # ========================================\n",
    "    # RecursiveCharacterTextSplitter splits text intelligently:\n",
    "    # - Tries to split at paragraph breaks first, then sentences, then words\n",
    "    # - Ensures chunks are approximately CHUNK_SIZE characters\n",
    "    # - Adds CHUNK_OVERLAP characters of overlap between chunks to preserve context\n",
    "    try:\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=CHUNK_SIZE,      # Target size: CHUNK_SIZE characters per chunk\n",
    "            chunk_overlap=CHUNK_OVERLAP  # Overlap: CHUNK_OVERLAP characters between chunks\n",
    "        )\n",
    "        # Split all documents into chunks\n",
    "        docs = text_splitter.split_documents(all_documents)\n",
    "        # What split_documents() does:\n",
    "            # 1. Reads each Document's page_content.\n",
    "            # 2. Splits text into ~CHUNK_SIZE-character chunks (paragraph/sentence boundaries preferred).\n",
    "            # 3. Creates a new Document object for each chunk.\n",
    "            # 4. Preserves original metadata (source, page, etc.).\n",
    "        # Result: docs = [chunk_1, chunk_2, chunk_3, ...]\n",
    "        # Each chunk is a Document object\n",
    "        print(f\"üîπ Total chunks created: {len(docs)}\")\n",
    "        logger.info(f\"Created {len(docs)} text chunks (size={CHUNK_SIZE}, overlap={CHUNK_OVERLAP})\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Error splitting documents: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Embedding and Vector Store Creation\n",
    "\n",
    "**What This Section Does:**\n",
    "Converts all document chunks into numerical vectors (embeddings) and builds a searchable index.\n",
    "\n",
    "**Why We Need Embeddings:**\n",
    "- Raw text cannot be directly compared by computers\n",
    "- Embeddings convert text into numerical vectors that capture **semantic meaning**\n",
    "- Similar texts ‚Üí similar vectors ‚Üí can search for relevant documents by comparing vectors\n",
    "\n",
    "**Process:**\n",
    "\n",
    "1. **Initialize Embedding Model**\n",
    "   - üî• Uses **BAAI/bge-m3** (multilingual¬∑multitask embedding, medium size)\n",
    "   - Based on sentence-transformers, so can be used directly with `HuggingFaceEmbeddings`\n",
    "   - Why? Lighter than gte-large, runs at realistic speeds on local Mac (MPS) environments while maintaining excellent search quality\n",
    "\n",
    "2. **Generate Vector Embeddings**\n",
    "   - Each document chunk is converted into a vector (high-dimensional numeric array)\n",
    "   - These vectors represent the **semantic meaning** of the text\n",
    "\n",
    "3. **Build Vector Store** (`ChromaDB`)\n",
    "   - Stores all embeddings for fast search\n",
    "   - Builds an index that enables similarity search\n",
    "   - Supports local persistent storage\n",
    "\n",
    "**Next Steps:**\n",
    "- When you ask a question, the query is also converted into an embedding\n",
    "- The system compares the query embedding against all document embeddings\n",
    "- Returns the most similar documents (based on cosine similarity score)\n",
    "\n",
    "**Key Advantages:**\n",
    "- Goes beyond simple keyword matching (understands that \"OLED\" and \"organic light-emitting diode\" are related)\n",
    "- Captures semantic relationships (e.g., \"phosphorescent material\" vs \"blue phosphorescence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Vector Store Construction (ChromaDB with Persistence)\n",
    "# This cell embeds documents and saves them to ChromaDB, or loads existing DB if it exists.\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# ========================================\n",
    "# STEP 1: Initialize Embedding Model\n",
    "# ========================================\n",
    "# üî• BAAI/bge-m3: Medium-sized embedding model for multilingual and multi-task use\n",
    "#    - sentence-transformers family ‚Üí Easy to use with HuggingFaceEmbeddings\n",
    "#    - Lighter than gte-large and realistic speed on Mac (MPS)\n",
    "EMBEDDING_MODEL = \"BAAI/bge-m3\"\n",
    "\n",
    "try:\n",
    "    print(\"üîÑ Loading embedding model...\")\n",
    "    print(f\"üì¶ Model: {EMBEDDING_MODEL}\")\n",
    "    \n",
    "    import torch\n",
    "    \n",
    "    # Device configuration\n",
    "    if torch.cuda.is_available():\n",
    "        device = 'cuda'\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = 'mps'\n",
    "    else:\n",
    "        device = 'cpu'\n",
    "    print(f\"‚úÖ Using device: {device.upper()}\")\n",
    "    \n",
    "    # Load BGE-m3 embedding model\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=EMBEDDING_MODEL,\n",
    "        model_kwargs={\n",
    "            'device': device,\n",
    "        },\n",
    "        encode_kwargs={\n",
    "            'normalize_embeddings': True,  # Normalize for cosine similarity\n",
    "            'batch_size': 16,              # üîë Batch size (adjustable between 8~32)\n",
    "        }\n",
    "    )\n",
    "    print(f\"‚úÖ Embedding model loaded! ({EMBEDDING_MODEL})\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to load embedding model: {e}\")\n",
    "    raise\n",
    "\n",
    "# ========================================\n",
    "# STEP 2: Load or Create ChromaDB (Check -> Exist?)\n",
    "# ========================================\n",
    "def get_vectorstore():\n",
    "    # Check if DB folder exists and is not empty\n",
    "    if os.path.exists(DB_PATH) and os.listdir(DB_PATH):\n",
    "        print(f\"\\nüìÇ Found existing ChromaDB at {DB_PATH}\")\n",
    "        print(\"üîÑ Loading existing DB... (skipping embedding)\")\n",
    "        \n",
    "        vectorstore = Chroma(\n",
    "            persist_directory=DB_PATH,\n",
    "            embedding_function=embeddings\n",
    "        )\n",
    "        print(\"‚úÖ Existing DB loaded!\")\n",
    "        return vectorstore\n",
    "    else:\n",
    "        print(f\"\\nüÜï No existing DB found. Embedding documents from scratch.\")\n",
    "        print(f\"üîÑ Embedding {len(docs)} chunks... (this may take time)\")\n",
    "        \n",
    "        # If empty folder exists, delete and recreate\n",
    "        if os.path.exists(DB_PATH):\n",
    "            shutil.rmtree(DB_PATH)\n",
    "            \n",
    "        vectorstore = Chroma.from_documents(\n",
    "            documents=docs,\n",
    "            embedding=embeddings,\n",
    "            persist_directory=DB_PATH\n",
    "        )\n",
    "        # Chroma automatically calls persist(), but explicitly save\n",
    "        # vectorstore.persist() # Latest version auto-saves\n",
    "        print(f\"‚úÖ New DB created and saved: {DB_PATH}\")\n",
    "        return vectorstore\n",
    "\n",
    "try:\n",
    "    vectorstore = get_vectorstore()\n",
    "    # Verify data\n",
    "    count = vectorstore._collection.count()\n",
    "    print(f\"üìä Number of document chunks stored in DB: {count}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error processing vector store: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Strict RAG System\n",
    "\n",
    "**What This Is:**\n",
    "This is the **core engine** of OLED Assistant. It's a Strict RAG system that uses documents as the **primary source** and only allows OLED/physics knowledge as supplementary when context is partial.\n",
    "\n",
    "**Problem It Solves:**\n",
    "- If answer is in documents ‚Üí Prioritize documents and answer with RAG (with limited LLM knowledge support if needed)\n",
    "- If relevant content is not in documents ‚Üí Clearly handle as \"No Answer\" or \"Information not in documents\"\n",
    "- Questions clearly unrelated to OLED ‚Üí Auto-reject (OFF_TOPIC)\n",
    "\n",
    "**How the Decision Process Works:**\n",
    "\n",
    "1. **Calculate Relevance Score (0.0 - 1.0)**\n",
    "   - Convert query to embedding\n",
    "   - Compare with document embeddings\n",
    "   - **Sigmoid transformation** amplifies separation:\n",
    "     - OLED-related queries ‚Üí boosted to 0.80‚Äì0.99 range\n",
    "     - Unrelated queries ‚Üí pushed to 0.01‚Äì0.20 range\n",
    "   - Average similarity after sigmoid = **Relevance Score**\n",
    "\n",
    "2. **Three-Tier Decision System:**\n",
    "\n",
    "   **TIER 1: RAG Mode** (Relevance ‚â• `RELEVANCE_THRESHOLD`)\n",
    "   - Documents are highly relevant\n",
    "   - ‚úÖ Use RAG directly (documents + LLM)\n",
    "   - Result: üü¢ Green answer\n",
    "\n",
    "   **TIER 2: No Information in Documents** (RAG returns \"Information not found\")\n",
    "   - Documents are relevant but no answer to specific question\n",
    "   - ‚úÖ Return \"Content not available\"\n",
    "   - Result: üü† Orange (information absence)\n",
    "\n",
    "   **TIER 3: Off-Topic Auto-Reject** (Relevance < `RELEVANCE_THRESHOLD`)\n",
    "   - Clearly unrelated to OLED display\n",
    "   - ‚úÖ Immediately reject (no LLM call ‚Üí saves cost)\n",
    "   - Result: üî¥ Red (auto-rejected)\n",
    "\n",
    "---\n",
    "\n",
    "### üìä Understanding the Score System\n",
    "\n",
    "**1. Relevance Score (0.0 ‚Äì 1.0)**\n",
    "- **Purpose**: \"Are the documents relevant to this query?\"\n",
    "- **Calculation Method:**\n",
    "  1. Query ‚Üí embedding vector\n",
    "  2. Compare with top-K document embeddings (cosine similarity)\n",
    "  3. Calculate average similarity\n",
    "  4. Apply sigmoid transformation (amplifies separation)\n",
    "- **Usage**: Decides which mode to use\n",
    "\n",
    "**2. RAG Quality Score (1‚Äì10)**\n",
    "- **Purpose**: \"How good is the RAG answer?\"\n",
    "- **Calculation Method:** LLM evaluates on 3 metrics and averages:\n",
    "  - **Specificity** (1‚Äì10): How specific is the answer?\n",
    "  - **Relevance** (1‚Äì10): Does it actually answer the question?\n",
    "  - **Factuality** (1‚Äì10): Does it contain verifiable facts and data?\n",
    "- **Usage**: RAG system performance evaluation\n",
    "\n",
    "**Key Distinction:**\n",
    "- **Relevance Score** = Quality of **document matching** (input quality)\n",
    "- **RAG Score** = Quality of the **answer itself** (output quality)\n",
    "- These are **independent** ‚Äì high relevance but low quality answers can occur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Strict RAG Advisor Implementation\n",
    "# This cell implements a Strict RAG system that answers only based on content in documents.\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "\n",
    "def create_llm(provider: str, model_name: str, temperature: float):\n",
    "    \"\"\"LLM factory function.\n",
    "    \n",
    "    - provider=\"openai\"  ‚Üí OpenAI official endpoint (e.g., gpt-4o-mini)\n",
    "    - provider=\"llama_local\" ‚Üí OpenAI-compatible local/on-premise endpoint (e.g., Ollama, vLLM, internal LLM server, etc.)\n",
    "    \n",
    "    By separating this, you can easily swap between OpenAI-based models ‚Üî local Mistral¬∑Llama¬∑Gemma etc.\n",
    "    by just changing provider / model_name later.\n",
    "    \"\"\"\n",
    "    if provider == \"openai\":\n",
    "        # Current setting: OpenAI API (e.g., gpt-4o-mini)\n",
    "        return ChatOpenAI(model=model_name, temperature=temperature)\n",
    "    elif provider == \"llama_local\":\n",
    "        # Local/on-premise OpenAI-compatible server (e.g., Ollama, vLLM)\n",
    "        # Default is Ollama OpenAI endpoint: http://localhost:11434/v1\n",
    "        base_url = os.getenv(\"LOCAL_LLM_BASE_URL\", os.getenv(\"LLAMA_BASE_URL\", \"http://localhost:11434/v1\"))\n",
    "        api_key = os.getenv(\"LOCAL_LLM_API_KEY\", os.getenv(\"LLAMA_API_KEY\", \"ollama\"))  # Ollama doesn't check tokens\n",
    "        return ChatOpenAI(\n",
    "            model=model_name,\n",
    "            temperature=temperature,\n",
    "            base_url=base_url,\n",
    "            api_key=api_key,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown LLM provider: {provider}\")\n",
    "\n",
    "\n",
    "class StrictRAGAdvisor:\n",
    "    \"\"\"Strict RAG System: Returns 'No Answer' if there's no answer in documents or question is unrelated.\"\"\"\n",
    "    \n",
    "    def __init__(self, vectorstore, llm_provider, llm_model, relevance_threshold, top_k, temperature, sigmoid_midpoint, sigmoid_steepness):\n",
    "        self.vectorstore = vectorstore\n",
    "        self.relevance_threshold = relevance_threshold\n",
    "        self.top_k = top_k\n",
    "        self.sigmoid_midpoint = sigmoid_midpoint\n",
    "        self.sigmoid_steepness = sigmoid_steepness\n",
    "        \n",
    "        # üîë LLM is created only here ‚Üí can swap later by just changing provider\n",
    "        self.llm = create_llm(provider=llm_provider, model_name=llm_model, temperature=temperature)\n",
    "        \n",
    "        # Strict RAG prompt (not completely blocking, prioritizes documents but allows supplementary knowledge)\n",
    "        rag_prompt_template = \"\"\"You are an OLED Display Technical Assistant.\n",
    "Answer the question using the provided context documents as your PRIMARY source.\n",
    "\n",
    "RULES:\n",
    "1. Always read the Context carefully and base your answer as much as possible on the Context.\n",
    "2. If the Context contains partial but relevant information, you MAY use your own OLED/physics knowledge to fill in missing logical steps.\n",
    "3. ONLY when the Context is clearly irrelevant or provides almost no signal, say: \"Information not found in the provided OLED documents.\"\n",
    "4. Never contradict the facts given in the Context.\n",
    "5. Do NOT hallucinate specific numbers, experimental conditions, or paper titles that are not supported by the Context.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        RAG_PROMPT = PromptTemplate(\n",
    "            template = rag_prompt_template,\n",
    "            input_variables = [\"context\", \"question\"]\n",
    "        )\n",
    "        \n",
    "        self.rag_chain = RetrievalQA.from_chain_type(\n",
    "            llm = self.llm,\n",
    "            chain_type = \"stuff\",\n",
    "            retriever = vectorstore.as_retriever(search_kwargs={\"k\": top_k}),\n",
    "            chain_type_kwargs = {\"prompt\": RAG_PROMPT}\n",
    "        )\n",
    "        \n",
    "    def get_relevance_score(self, query):\n",
    "        # ChromaDB returns distance, so need to convert to similarity\n",
    "        # Chroma L2 distance: lower is better (0 = identical)\n",
    "        # Similarity = 1 - distance (assuming normalized embeddings)\n",
    "        # But need to check what value langchain wrapper gives in similarity_search_with_score\n",
    "        # LangChain Chroma's similarity_search_with_score returns L2 distance by default\n",
    "        \n",
    "        docs_with_scores = self.vectorstore.similarity_search_with_score(query, k=self.top_k)\n",
    "        if not docs_with_scores:\n",
    "            return 0.0\n",
    "        \n",
    "        # Score conversion (L2 distance -> Similarity)\n",
    "        # If distance is 0, similarity is 1; if distance is 1+, similarity is low\n",
    "        # Safely: similarity = 1.0 / (1.0 + distance)\n",
    "        scores = []\n",
    "        for doc, score in docs_with_scores:\n",
    "             # Chroma returns distance by default. Lower is better.\n",
    "             # Convert to 0-1 similarity score\n",
    "             similarity = 1.0 / (1.0 + score)\n",
    "             scores.append(similarity)\n",
    "        \n",
    "        avg_score = sum(scores) / len(scores)\n",
    "        \n",
    "        # Sigmoid transformation to spread score distribution\n",
    "        sigmoid_score = 1 / (1 + math.exp(-self.sigmoid_steepness * (avg_score - self.sigmoid_midpoint)))\n",
    "        return sigmoid_score\n",
    "    \n",
    "    def query(self, question):\n",
    "        # Strict RAG Logic\n",
    "        relevance_score = self.get_relevance_score(question)\n",
    "        \n",
    "        result = {\n",
    "            \"answer\": None, \n",
    "            \"mode\": None, \n",
    "            \"relevance_score\": relevance_score,\n",
    "            \"retrieved_docs\": []\n",
    "        }\n",
    "        \n",
    "        # Check threshold (reject immediately if document similarity is low)\n",
    "        if relevance_score >= self.relevance_threshold:\n",
    "            result[\"mode\"] = \"RAG\"\n",
    "            result[\"retrieved_docs\"] = self.vectorstore.similarity_search(question, k=self.top_k)\n",
    "            # Execute RAG Chain (prompt instructs to say 'no information' if not found)\n",
    "            rag_response = self.rag_chain.invoke({\"query\": question})[\"result\"]\n",
    "            result[\"answer\"] = rag_response\n",
    "            \n",
    "            # Secondary check if LLM returned 'no information'\n",
    "            if \"Information not found\" in rag_response or \"provided context\" in rag_response and \"does not contain\" in rag_response:\n",
    "                 result[\"mode\"] = \"NO_ANSWER_IN_DOCS\"\n",
    "                 result[\"answer\"] = \"No Answer: The relevant content is not found in RAG documents.\"\n",
    "        else:\n",
    "            result[\"mode\"] = \"OFF_TOPIC\"\n",
    "            result[\"answer\"] = \"No Answer: The question is not related to OLED display or relevant documents are not available.\"\n",
    "            logger.info(f\"üö´ Low relevance rejection: {relevance_score:.3f}\")\n",
    "            \n",
    "        return result\n",
    "\n",
    "# Initialize Strict Advisor\n",
    "advisor = StrictRAGAdvisor(\n",
    "    vectorstore = vectorstore,\n",
    "    llm_provider = LLM_PROVIDER,  # üîë Will be replaced with \"llama_local\" later\n",
    "    llm_model = LLM_MODEL,\n",
    "    relevance_threshold = RELEVANCE_THRESHOLD,\n",
    "    top_k = TOP_K_DOCUMENTS,\n",
    "    temperature = LLM_TEMPERATURE,\n",
    "    sigmoid_midpoint = SIGMOID_MIDPOINT,\n",
    "    sigmoid_steepness = SIGMOID_STEEPNESS\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Strict RAG Advisor (OLED) initialized successfully!\")\n",
    "print(f\"üìä Strict Threshold = {RELEVANCE_THRESHOLD}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Enhanced Query Function with Monitoring\n",
    "\n",
    "**What This Section Does:**\n",
    "Wraps the advisor's `query` method with automatic monitoring, logging, and error handling.\n",
    "\n",
    "**Why It's Needed:**\n",
    "- Track performance metrics (response time, cost, token usage)\n",
    "- Handle errors gracefully (API failures, timeouts)\n",
    "- Log experiment data for hyperparameter tuning\n",
    "- Systematically compare different configurations\n",
    "\n",
    "**What Gets Tracked:**\n",
    "- ‚è±Ô∏è **Response Time**: Time each query takes (seconds)\n",
    "- üí∞ **Token Usage & Cost**: Input/output tokens and estimated API cost\n",
    "- üìä **Mode Distribution**: Number of queries per mode (RAG, NO_ANSWER, OFF_TOPIC, etc.)\n",
    "- ‚ö†Ô∏è **Errors & Retries**: Automatic retry on transient failures (up to 2 attempts)\n",
    "\n",
    "**How It Works:**\n",
    "- The `monitored_query()` function wraps `advisor.query()`\n",
    "- All metrics are automatically logged to the experiment tracker\n",
    "- Returns the same structure as `advisor.query()` plus extra metadata\n",
    "- The `ask()` helper provides a convenient one-line interface for quick queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16: Enhanced Query Function with Monitoring\n",
    "# This cell creates a wrapper function that adds monitoring, error handling, and cost tracking\n",
    "\n",
    "def monitored_query(advisor, question, track_cost=True, log_to_experiment=True):\n",
    "    \"\"\"\n",
    "    Enhanced query function with automatic monitoring and error handling.\n",
    "    \n",
    "    What This Function Does:\n",
    "    1. Wraps advisor.query() with timing, error handling, and retry logic\n",
    "    2. Tracks token usage and calculates estimated API cost\n",
    "    3. Logs query results to experiment tracker for analysis\n",
    "    4. Returns same format as advisor.query() + additional metadata\n",
    "    \n",
    "    Args:\n",
    "        advisor: StrictRAGAdvisor instance (the main advisor object for OLED RAG)\n",
    "        question: User's question text\n",
    "        track_cost: Whether to track token usage and cost (default: True)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Result dictionary with answer, metadata, response_time, token counts\n",
    "    \"\"\"\n",
    "    start_time = time.time()  # Start timer for response time calculation\n",
    "    result = None\n",
    "    error_occurred = False\n",
    "    \n",
    "    try:\n",
    "        logger.info(f\"üîç Processing query: {question[:50]}...\")\n",
    "        \n",
    "        # Wrap advisor.query() with automatic retry on API errors\n",
    "        # @retry_on_api_error automatically retries up to 2 times on failures\n",
    "        @retry_on_api_error(max_retries=2, delay=2)\n",
    "        def query_with_retry():\n",
    "            return advisor.query(question)\n",
    "        \n",
    "        # Call the wrapped query method\n",
    "        result = query_with_retry()\n",
    "        \n",
    "        # Calculate response time (elapsed time since start)\n",
    "        response_time = time.time() - start_time\n",
    "        result['response_time'] = response_time\n",
    "        \n",
    "        # Track token usage and cost if enabled\n",
    "        if track_cost:\n",
    "            # Estimate token usage based on input/output text length\n",
    "            # cost_tracker uses tiktoken to count tokens accurately\n",
    "            input_text = question\n",
    "            output_text = result['answer']\n",
    "            input_tokens, output_tokens = cost_tracker.add_tokens(input_text, output_text)\n",
    "            \n",
    "            # Store token counts in result for display/analysis\n",
    "            result['input_tokens'] = input_tokens\n",
    "            result['output_tokens'] = output_tokens\n",
    "        \n",
    "        # Log successful query completion with key metrics\n",
    "        mode = result['mode']\n",
    "        logger.info(f\"‚úÖ Query completed in {response_time:.2f}s | Mode: {mode} | Relevance: {result['relevance_score']:.3f}\")\n",
    "        \n",
    "        # Track this query in experiment tracker for statistical analysis\n",
    "        # This accumulates data for comparing different hyperparameter settings\n",
    "        if log_to_experiment and hasattr(experiment_tracker, 'current_experiment') and experiment_tracker.current_experiment:\n",
    "            experiment_tracker.log_query(response_time, mode)\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Error handling: Catch any exceptions and return error result\n",
    "        error_occurred = True\n",
    "        response_time = time.time() - start_time\n",
    "        \n",
    "        logger.error(f\"‚ùå Query failed after {response_time:.2f}s: {type(e).__name__} - {str(e)}\")\n",
    "        \n",
    "        # Return error result with user-friendly message\n",
    "        return {\n",
    "            'mode': 'ERROR',\n",
    "            'answer': f\"Sorry, an error occurred while processing your question: {str(e)}\",\n",
    "            'relevance_score': 0.0,\n",
    "            'response_time': response_time,\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "# Create a convenience function that uses the global advisor\n",
    "def ask(question):\n",
    "    \"\"\"\n",
    "    Convenient shortcut function to ask a question with automatic monitoring.\n",
    "    \n",
    "    This is a simpler interface - just call ask(question) instead of \n",
    "    monitored_query(advisor, question).\n",
    "    \n",
    "    Usage:\n",
    "        result = ask(\"What are the key degradation mechanisms of a blue phosphorescent OLED?\")\n",
    "        print(result['answer'])\n",
    "    \n",
    "    Args:\n",
    "        question: User's question text\n",
    "    \n",
    "    Returns:\n",
    "        dict: Result dictionary with answer and metadata\n",
    "    \"\"\"\n",
    "    return monitored_query(advisor, question)\n",
    "\n",
    "print(\"‚úÖ Monitored query function ready!\")\n",
    "print(\"üí° Use ask(question) for quick queries with automatic monitoring\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Testing the Strict RAG System\n",
    "\n",
    "Test the advisor with sample questions to see how RAG, NO_ANSWER, and OFF_TOPIC modes are selected.\n",
    "\n",
    "---\n",
    "### üìä Quick Reference: The Score System\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  RELEVANCE SCORE (0.0 - 1.0)                                    ‚îÇ\n",
    "‚îÇ  Question: \"Do the documents match the query?\"                  ‚îÇ\n",
    "‚îÇ  ‚Ä¢ 0.85 ‚Üí Documents are highly relevant to the query ‚úì          ‚îÇ\n",
    "‚îÇ  ‚Ä¢ 0.54 ‚Üí Documents somewhat match (uncertain zone)             ‚îÇ\n",
    "‚îÇ  ‚Ä¢ 0.30 ‚Üí Documents are not relevant to the query ‚úó             ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  RAG SCORE (1-10)                                               ‚îÇ\n",
    "‚îÇ  Question: \"How good is the RAG answer?\"                        ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Measures: Specificity + Relevance + Factuality               ‚îÇ\n",
    "‚îÇ  ‚Ä¢ 8.5 ‚Üí High-quality answer with concrete facts from docs      ‚îÇ\n",
    "‚îÇ  ‚Ä¢ 6.0 ‚Üí Decent answer but lacks detail                         ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "üí° KEY INSIGHTS:\n",
    "   1. Low Relevance = No OLED-related documents ‚Üí OFF_TOPIC rejection\n",
    "   2. High Relevance + RAG \"Information not found\" = Relevant documents exist but no specific answer ‚Üí NO_ANSWER\n",
    "   3. High Relevance + Successful answer = Accurate document-based answer ‚Üí RAG mode\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 18: Execute Advisor Core Logic with Test Questions\n",
    "# Execute advisor with predefined test questions and format the results for output\n",
    "\n",
    "# HTML formatting function - for better-looking result output\n",
    "def show_response(result, question):\n",
    "    \"\"\"\n",
    "    Display advisor response in a formatted HTML box.\n",
    "    Applied for Strict RAG mode (Project 2).\n",
    "    \"\"\"\n",
    "    mode = result['mode']\n",
    "    answer = result['answer']\n",
    "    relevance_score = result['relevance_score']\n",
    "    \n",
    "    # Color coding by mode (Strict RAG: Only use RAG, NO_ANSWER_IN_DOCS, OFF_TOPIC)\n",
    "    if mode == \"RAG\":\n",
    "        color = \"#4caf50\"  # Green\n",
    "        icon = \"üü¢\"\n",
    "        mode_text = \"RAG Mode (Document-based Response)\"\n",
    "    elif mode == \"NO_ANSWER_IN_DOCS\":\n",
    "        color = \"#ff9800\"  # Orange\n",
    "        icon = \"üü†\"\n",
    "        mode_text = \"No Answer (No Relevant Content in Documents)\"\n",
    "    elif mode == \"OFF_TOPIC\":\n",
    "        color = \"#f44336\"  # Red\n",
    "        icon = \"üî¥\"\n",
    "        mode_text = \"Off-Topic Rejection (Low Relevance, Auto-Rejected)\"\n",
    "    elif mode == \"ERROR\":\n",
    "        color = \"#f44336\"  # Red\n",
    "        icon = \"‚ùå\"\n",
    "        mode_text = \"System Error\"\n",
    "    else:\n",
    "        color = \"#9e9e9e\"  # Gray\n",
    "        icon = \"‚ö™\"\n",
    "        mode_text = f\"Unknown mode: {mode}\"\n",
    "    \n",
    "    # Generate metadata line (includes relevance score)\n",
    "    metadata = f\"{icon} <strong>{mode_text}</strong> | Relevance Score: {relevance_score:.3f}\"\n",
    "    \n",
    "    html = f\"\"\"\n",
    "    <div style=\"\n",
    "        background-color:#f9f9f9;\n",
    "        border-left: 6px solid {color};\n",
    "        padding: 15px;\n",
    "        margin: 15px 0;\n",
    "        border-radius: 4px;\n",
    "        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Arial, sans-serif;\">\n",
    "        <div style=\"color:#333; font-size:14px; margin-bottom:10px;\">\n",
    "            <strong>Question:</strong> {question}\n",
    "        </div>\n",
    "        <div style=\"color:#666; font-size:13px; margin-bottom:10px;\">\n",
    "            {metadata}\n",
    "        </div>\n",
    "        <div style=\"color:#111; font-size:14px; line-height:1.6; white-space: pre-wrap;\">\n",
    "            <strong>Answer:</strong><br>{answer}\n",
    "        </div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    display(HTML(html))\n",
    "\n",
    "# Execute advisor core logic with test questions\n",
    "print(\"üß™ Executing advisor core logic with test questions...\\n\")\n",
    "\n",
    "# Start experiment tracking\n",
    "experiment_tracker.start_experiment({\n",
    "    # RAG Quality (Step 1)\n",
    "    'chunk_size': CHUNK_SIZE,\n",
    "    'chunk_overlap': CHUNK_OVERLAP,\n",
    "    'top_k': TOP_K_DOCUMENTS,\n",
    "    'temperature': LLM_TEMPERATURE,\n",
    "    \n",
    "    # System Architecture (Step 2) - Only what's used in Strict RAG\n",
    "    'relevance_threshold': RELEVANCE_THRESHOLD,\n",
    "    'sigmoid_midpoint': SIGMOID_MIDPOINT,\n",
    "    'sigmoid_steepness': SIGMOID_STEEPNESS\n",
    "})\n",
    "\n",
    "# Reset cost tracker for this experiment\n",
    "cost_tracker.reset()\n",
    "\n",
    "# Store relevance scores for later average calculation\n",
    "relevance_scores = []\n",
    "\n",
    "for i, question in enumerate(TEST_QUERIES, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Test {i}/{len(TEST_QUERIES)}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Use monitored_query() instead of advisor.query()\n",
    "    result = monitored_query(advisor, question)\n",
    "    show_response(result, question)\n",
    "    \n",
    "    # Save relevance score\n",
    "    if 'relevance_score' in result:\n",
    "        relevance_scores.append(result['relevance_score'])\n",
    "    \n",
    "    # Display performance metrics\n",
    "    if 'response_time' in result:\n",
    "        print(f\"‚è±Ô∏è Response time: {result['response_time']:.2f}s\")\n",
    "    if 'input_tokens' in result and 'output_tokens' in result:\n",
    "        print(f\"üé´ Tokens: {result['input_tokens']} input + {result['output_tokens']} output = {result['input_tokens'] + result['output_tokens']} total\")\n",
    "\n",
    "# Print experiment summary\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"üìä Experiment Summary\")\n",
    "print(f\"{'='*80}\")\n",
    "cost_summary = cost_tracker.get_summary()\n",
    "print(f\"üí∞ Total tokens: {cost_summary['total_tokens']:,}\")\n",
    "print(f\"üíµ Estimated cost: ${cost_summary['estimated_cost_usd']:.4f}\")\n",
    "print(f\"üìä Mode distribution:\")\n",
    "for mode, count in experiment_tracker.current_experiment['mode_counts'].items():\n",
    "    if count > 0:\n",
    "        print(f\"   - {mode}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Quantitative Evaluation: System Quality\n",
    "\n",
    "**What This Section Does:**\n",
    "Evaluates the quality of answers generated by the Strict RAG system.\n",
    "\n",
    "**Evaluation Process:**\n",
    "1. **For each test question:**\n",
    "   - Generate answer using Strict RAG Advisor\n",
    "   - If answer is provided (not rejected), score it using LLM-as-a-judge\n",
    "\n",
    "2. **Evaluation Metrics** (1-10 points):\n",
    "   - **Specificity**: How detailed is the answer?\n",
    "   - **Relevance**: Does it directly answer the question?\n",
    "   - **Factuality**: Does it contain verifiable facts from the documents?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 20: Quantitative Evaluation - System Quality Check\n",
    "# This cell implements the RAGEvaluator class for evaluating system performance.\n",
    "# Note: LLM baseline comparison has been removed to focus on Strict RAG quality.\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "class RAGEvaluator:\n",
    "    \"\"\"Evaluate the final advisor answer quality.\"\"\"\n",
    "    \n",
    "    def __init__(self, llm, advisor):\n",
    "        self.llm = llm\n",
    "        self.advisor = advisor\n",
    "    \n",
    "    def get_final_system_answer(self, question):\n",
    "        \"\"\"Get the final answer from the full advisor system.\"\"\"\n",
    "        # track_cost=False, log_to_experiment=False to avoid double counting\n",
    "        result = monitored_query(self.advisor, question, track_cost=False, log_to_experiment=False)\n",
    "        return result\n",
    "    \n",
    "    def score_answer(self, question, answer):\n",
    "        \"\"\"Score an answer on specificity, relevance, and factuality (1-10).\"\"\"\n",
    "        evaluation_prompt = f\"\"\"You are an expert evaluator. Score the following answer on these three criteria (scale 1-10):\n",
    "\n",
    "1. SPECIFICITY: How specific and detailed is the answer? (1 = vague, 10 = very specific with details)\n",
    "2. RELEVANCE: How relevant is the answer to the question? (1 = off-topic, 10 = directly answers question)\n",
    "3. FACTUALITY: Does the answer contain verifiable facts and data? (1 = no facts/opinions only, 10 = rich with facts and data)\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: {answer}\n",
    "\n",
    "Respond ONLY with a JSON object in this exact format (no other text):\n",
    "{{\"specificity\": <score>, \"relevance\": <score>, \"factuality\": <score>}}\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.llm.invoke(evaluation_prompt).content\n",
    "            start_idx = response.find('{')\n",
    "            end_idx = response.rfind('}') + 1\n",
    "            json_str = response[start_idx:end_idx]\n",
    "            scores = json.loads(json_str)\n",
    "            return scores\n",
    "        except:\n",
    "            return {\"specificity\": 5, \"relevance\": 5, \"factuality\": 5}\n",
    "    \n",
    "    def evaluate_system(self, question):\n",
    "        \"\"\"Evaluate final system answer quality.\"\"\"\n",
    "        result = self.get_final_system_answer(question)\n",
    "        final_answer = result.get(\"answer\", \"\")\n",
    "        mode = result.get(\"mode\", \"UNKNOWN\")\n",
    "        \n",
    "        # If no answer (rejected), we can't score specificity/factuality nicely\n",
    "        # But we can track that it was rejected.\n",
    "        if mode in [\"OFF_TOPIC\", \"NO_ANSWER_IN_DOCS\"]:\n",
    "             return {\n",
    "                \"question\": question,\n",
    "                \"final_answer\": final_answer,\n",
    "                \"mode\": mode,\n",
    "                \"final_specificity\": 0,\n",
    "                \"final_relevance\": 0,\n",
    "                \"final_factuality\": 0,\n",
    "                \"final_word_count\": 0\n",
    "            }\n",
    "            \n",
    "        # Score the answer\n",
    "        final_scores = self.score_answer(question, final_answer)\n",
    "        final_word_count = len(final_answer.split())\n",
    "        \n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"final_answer\": final_answer,\n",
    "            \"mode\": mode,\n",
    "            \"final_specificity\": final_scores[\"specificity\"],\n",
    "            \"final_relevance\": final_scores[\"relevance\"],\n",
    "            \"final_factuality\": final_scores[\"factuality\"],\n",
    "            \"final_word_count\": final_word_count,\n",
    "        }\n",
    "\n",
    "# Initialize evaluator\n",
    "evaluator = RAGEvaluator(advisor.llm, advisor)\n",
    "\n",
    "# Run evaluation\n",
    "print(\"üìä Running system quality evaluation...\\n\")\n",
    "evaluation_results = []\n",
    "\n",
    "for i, question in enumerate(TEST_QUERIES, 1):\n",
    "    print(f\"Evaluating {i}/{len(TEST_QUERIES)}: {question[:50]}...\")\n",
    "    result = evaluator.evaluate_system(question)\n",
    "    evaluation_results.append(result)\n",
    "    print(\"  ‚úì Complete\")\n",
    "\n",
    "print(\"\\n‚úÖ Evaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Evaluation Results Analysis\n",
    "This section analyzes the performance of the Strict RAG system, calculating quality scores for answered questions and tracking rejection counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 22: System Evaluation Analysis\n",
    "# This cell displays performance metrics for the Strict RAG system.\n",
    "\n",
    "# Convert results to DataFrame\n",
    "df = pd.DataFrame(evaluation_results)\n",
    "\n",
    "# Filter only answered questions for scoring stats (exclude rejections)\n",
    "answered_df = df[~df['mode'].isin(['OFF_TOPIC', 'NO_ANSWER_IN_DOCS'])]\n",
    "\n",
    "if not answered_df.empty:\n",
    "    avg_specificity = answered_df['final_specificity'].mean()\n",
    "    avg_relevance = answered_df['final_relevance'].mean()\n",
    "    avg_factuality = answered_df['final_factuality'].mean()\n",
    "    avg_words = answered_df['final_word_count'].mean()\n",
    "    \n",
    "    # Calculate overall score\n",
    "    answered_df['final_overall'] = (answered_df['final_specificity'] + answered_df['final_relevance'] + answered_df['final_factuality']) / 3\n",
    "    avg_overall = answered_df['final_overall'].mean()\n",
    "else:\n",
    "    avg_specificity = 0\n",
    "    avg_relevance = 0\n",
    "    avg_factuality = 0\n",
    "    avg_words = 0\n",
    "    avg_overall = 0\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"=\" * 80)\n",
    "print(\"üìä STRICT RAG SYSTEM PERFORMANCE SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total Questions: {len(df)}\")\n",
    "print(f\"Answered: {len(answered_df)}\")\n",
    "print(f\"Rejected/No Data: {len(df) - len(answered_df)}\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Metric (Answered Queries Only)':<35} {'Score':<15}\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Specificity (1-10)':<35} {avg_specificity:<15.2f}\")\n",
    "print(f\"{'Relevance (1-10)':<35} {avg_relevance:<15.2f}\")\n",
    "print(f\"{'Factuality (1-10)':<35} {avg_factuality:<15.2f}\")\n",
    "print(f\"{'Overall Score (1-10)':<35} {avg_overall:<15.2f}\")\n",
    "print(f\"{'Average Word Count':<35} {avg_words:<15.1f}\")\n",
    "print()\n",
    "\n",
    "# Display detailed results table\n",
    "print(\"=\" * 80)\n",
    "print(\"üìã DETAILED RESULTS BY QUESTION\")\n",
    "print(\"=\" * 80)\n",
    "display(df[['question', 'mode', 'final_answer']])\n",
    "\n",
    "# Calculate average relevance score from Cell 18's data\n",
    "# relevance_scores list is collected from Cell 18\n",
    "if 'relevance_scores' in dir() and len(relevance_scores) > 0:\n",
    "    avg_relevance_score = sum(relevance_scores) / len(relevance_scores)\n",
    "else:\n",
    "    avg_relevance_score = None\n",
    "    print(\"‚ö†Ô∏è Warning: relevance_scores not found. Run Cell 18 first.\")\n",
    "\n",
    "# Calculate Q1‚ÄìQ2 vs Q5‚ÄìQ6 separation (for CSV recording)\n",
    "relevance_mean_pos = None\n",
    "relevance_mean_neg = None\n",
    "relevance_gap = None\n",
    "relevance_margin = None\n",
    "\n",
    "try:\n",
    "    if 'relevance_scores' in dir() and len(relevance_scores) >= 6:\n",
    "        # Q1, Q2 ‚Üí OLED core questions (Positive)\n",
    "        pos_indices = [0, 1]\n",
    "        # Q5, Q6 ‚Üí Completely OFF-TOPIC questions (Negative)\n",
    "        neg_indices = [4, 5]\n",
    "\n",
    "        pos_scores = [float(relevance_scores[i]) for i in pos_indices]\n",
    "        neg_scores = [float(relevance_scores[i]) for i in neg_indices]\n",
    "\n",
    "        relevance_mean_pos = sum(pos_scores) / len(pos_scores)\n",
    "        relevance_mean_neg = sum(neg_scores) / len(neg_scores)\n",
    "        relevance_gap = relevance_mean_pos - relevance_mean_neg\n",
    "        relevance_margin = min(pos_scores) - max(neg_scores)\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Need at least 6 relevance scores (Q1‚ÄìQ6) to compute separation metrics.\")\n",
    "except Exception as e:\n",
    "    print(\"‚ö†Ô∏è Failed to compute relevance separation metrics:\", e)\n",
    "\n",
    "# Save experiment results\n",
    "if hasattr(experiment_tracker, 'save_experiment'):\n",
    "    # notes: Store simple summary text (also record which LLM configuration)\n",
    "    notes_str = (\n",
    "        f\"model={LLM_MODEL}, provider={LLM_PROVIDER}, \"\n",
    "        f\"Strict RAG Evaluation: {len(answered_df)}/{len(df)} answered.\"\n",
    "    )\n",
    "\n",
    "    experiment_tracker.save_experiment(\n",
    "        avg_relevance_score=avg_relevance_score,\n",
    "        avg_rag_score=avg_overall,\n",
    "        avg_rag_specificity=avg_specificity,\n",
    "        avg_rag_relevance=avg_relevance,\n",
    "        avg_rag_factuality=avg_factuality,\n",
    "        rel_mean_pos=relevance_mean_pos,\n",
    "        rel_mean_neg=relevance_mean_neg,\n",
    "        rel_gap=relevance_gap,\n",
    "        rel_margin=relevance_margin,\n",
    "        notes=notes_str,\n",
    "    )\n",
    "\n",
    "    # Console output always displays in the same format\n",
    "    mean_pos_str = \"N/A\" if relevance_mean_pos is None else f\"{relevance_mean_pos:.3f}\"\n",
    "    mean_neg_str = \"N/A\" if relevance_mean_neg is None else f\"{relevance_mean_neg:.3f}\"\n",
    "    gap_str = \"N/A\" if relevance_gap is None else f\"{relevance_gap:.3f}\"\n",
    "    margin_str = \"N/A\" if relevance_margin is None else f\"{relevance_margin:.3f}\"\n",
    "\n",
    "    print(f\"\\nüìä Average Relevance Score: {avg_relevance_score:.3f}\" if avg_relevance_score else \"\")\n",
    "    print(\n",
    "        f\"üìä Relevance separation (Q1‚ÄìQ2 vs Q5‚ÄìQ6): \"\n",
    "        f\"mean_pos={mean_pos_str}, \"\n",
    "        f\"mean_neg={mean_neg_str}, \"\n",
    "        f\"gap={gap_str}, margin={margin_str}\"\n",
    "    )\n",
    "    print(\"‚úÖ Experiment results saved to CSV.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí¨ Interactive Q&A Interface\n",
    "Now you can ask the AI-Driven OLED Assistant your own questions directly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 26: Interactive Q&A Interface\n",
    "# ‚ö†Ô∏è This cell is handled so handlers don't get registered multiple times even if run multiple times\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import clear_output, display, HTML\n",
    "\n",
    "# Global flag to prevent duplicate registration\n",
    "if 'OLED_UI_INITIALIZED' not in dir():\n",
    "    OLED_UI_INITIALIZED = False\n",
    "\n",
    "# Remove existing UI if present\n",
    "if OLED_UI_INITIALIZED:\n",
    "    try:\n",
    "        oled_ui_box.close()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Create widgets\n",
    "oled_question_input = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder='Enter your OLED Display related question...',\n",
    "    description='Question:',\n",
    "    layout=widgets.Layout(width='90%', height='80px'),\n",
    "    style={'description_width': '50px'}\n",
    ")\n",
    "\n",
    "oled_submit_btn = widgets.Button(\n",
    "    description='üîç Get Answer',\n",
    "    button_style='primary',\n",
    "    layout=widgets.Layout(width='150px')\n",
    ")\n",
    "\n",
    "oled_output = widgets.Output()\n",
    "\n",
    "def oled_handle_submit(btn):\n",
    "    with oled_output:\n",
    "        clear_output(wait=True)\n",
    "        question = oled_question_input.value.strip()\n",
    "        if not question:\n",
    "            print(\"‚ö†Ô∏è Please enter a question!\")\n",
    "            return\n",
    "        \n",
    "        print(\"üîç Analyzing question...\")\n",
    "        result = advisor.query(question)\n",
    "        \n",
    "        mode = result['mode']\n",
    "        answer = result['answer']\n",
    "        score = result['relevance_score']\n",
    "        \n",
    "        if mode == \"RAG\":\n",
    "            color, icon, text = \"#4caf50\", \"üü¢\", \"RAG Mode (Document-based)\"\n",
    "        elif mode == \"NO_ANSWER_IN_DOCS\":\n",
    "            color, icon, text = \"#ff9800\", \"üü†\", \"No Answer (Not in Documents)\"\n",
    "        else:\n",
    "            color, icon, text = \"#f44336\", \"üî¥\", \"Off-Topic (Rejected)\"\n",
    "        \n",
    "        html = f'''<div style=\"background:#f5f5f5; border-left:5px solid {color}; padding:12px; margin:8px 0; border-radius:4px;\">\n",
    "<b>Q:</b> {question}<br>\n",
    "<span style=\"color:#666\">{icon} {text} | Relevance: {score:.3f}</span><br><br>\n",
    "<b>A:</b> {answer}\n",
    "</div>'''\n",
    "        display(HTML(html))\n",
    "        \n",
    "        if mode == 'RAG' and result.get('retrieved_docs'):\n",
    "            print(\"\\nüìÑ Reference Documents:\")\n",
    "            for i, doc in enumerate(result['retrieved_docs'][:2], 1):\n",
    "                print(f\"  {i}. {doc.page_content[:120]}...\")\n",
    "\n",
    "oled_submit_btn.on_click(oled_handle_submit)\n",
    "\n",
    "# Wrap in UI box\n",
    "oled_ui_box = widgets.VBox([\n",
    "    widgets.HTML(\"<h3>üí¨ AI-Driven OLED Assistant</h3>\"),\n",
    "    oled_question_input,\n",
    "    oled_submit_btn,\n",
    "    oled_output\n",
    "])\n",
    "\n",
    "OLED_UI_INITIALIZED = True\n",
    "display(oled_ui_box)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
