{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI-Driven OLED Assistant (Domain-Specific RAG)\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "**Goal:** Build an intelligent AI assistant for OLED display engineering and technical support.\n",
    "\n",
    "**Key Features:**\n",
    "- Strict RAG: Document-based answers only; rejects queries when relevant documents are not found\n",
    "- Relevance Score: 3-tier decision system (RAG / NO_ANSWER / OFF_TOPIC) based on similarity + sigmoid transformation\n",
    "- Quantitative Evaluation: LLM-as-a-judge scoring for answer quality (Specificity / Relevance / Factuality)\n",
    "- Engineering Support: Answers questions about OLED processes, device properties, and optical simulations\n",
    "\n",
    "---\n",
    "## Required Packages\n",
    "\n",
    "### Core Dependencies\n",
    "- **langchain**: LangChain framework (RAG pipeline construction)\n",
    "- **langchain-community**: PDF/DOCX loaders and integrations\n",
    "- **langchain-openai**: Chat API wrapper for OpenAI and OpenAI-compatible servers (e.g., Ollama/vLLM)\n",
    "- **docarray**: Python vector search engine (lightweight and fast)\n",
    "- **pypdf**: PDF text extraction\n",
    "- **docx2txt**: DOCX text extraction\n",
    "- **tiktoken**: Token counting (for cost estimation)\n",
    "- **python-dotenv**: Environment variable management (.env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1\n",
    "# %pip install --upgrade pip\n",
    "# %pip install -U langchain langchain-community langchain-openai\n",
    "# %pip install -U docarray pypdf docx2txt tiktoken openai python-dotenv\n",
    "# %pip install -U sentence-transformers  # üîë For HuggingFace embeddings (sentence-transformers/all-MiniLM-L6-v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Variables and API Key Configuration\n",
    "\n",
    "**What This Section Does:**\n",
    "- Loads environment variables from the `.env` file in the project directory\n",
    "- By default, looks for `OPENAI_API_KEY`, but **the current Strict RAG experiment uses local Mistral-Nemo (Ollama)**, so it's not required\n",
    "- Prepares the key for future OpenAI-based LLM evaluation (e.g., gpt-4o-mini)\n",
    "- The `python-dotenv` package automatically reads the `.env` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load OpenAI API Key from .env file\n",
    "# This cell loads environment variables (especially the API key) before any API calls\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load all variables from .env file into environment\n",
    "# The .env file should be in the same directory as this notebook\n",
    "load_dotenv()\n",
    "\n",
    "# Check if API key was successfully loaded\n",
    "if os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    print(\"‚úÖ OpenAI API key loaded successfully from .env file\")\n",
    "else:\n",
    "    print(\"‚ùå Warning: OPENAI_API_KEY not found in .env file\")\n",
    "    print(\"   Please create a .env file with: OPENAI_API_KEY=your-key-here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration (Hyperparameters)\n",
    "\n",
    "These are the main hyperparameters you can adjust to customize the OLED Assistant system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Hyperparameters\n",
    "# =======================================\n",
    "# Hyperparameter Tuning Strategy (Strict RAG):\n",
    "# =======================================\n",
    "# Step 1: RAG Answer Quality Optimization\n",
    "#   - Tuning parameters: CHUNK_SIZE, CHUNK_OVERLAP, TOP_K_DOCUMENTS\n",
    "#\n",
    "# Step 2: Strict RAG Threshold Settings\n",
    "#   - Tuning parameters: RELEVANCE_THRESHOLD\n",
    "#   - Goal: Return 'No Answer' if information is not in the documents\n",
    "\n",
    "DOCS_FOLDER = \"../data\"  # Folder containing OLED technical documents\n",
    "DB_PATH = \"../chroma_db\" # Vector DB storage path (Persistence)\n",
    "# LLM Configuration: Using local Mistral-Nemo 12B (Ollama)\n",
    "LLM_MODEL = \"mistral-nemo\"      # Must match Ollama model name\n",
    "LLM_PROVIDER = \"llama_local\"    # Local OpenAI-compatible server\n",
    "# ========================================\n",
    "# STEP 1: RAG Answer Quality Settings\n",
    "# ========================================\n",
    "# CHUNK_SIZE: Chunk size (in characters)\n",
    "#    - Recommended range: 1500 ~ 4000 (600 ~ 8000)\n",
    "#    - Smaller: Precise search, but risk of context fragmentation\n",
    "#    - Larger: Context preservation, but risk of topic mixing\n",
    "#    - Technical docs/papers: 2000~3000 recommended (~1 page)\n",
    "#    - gte-large-en-v1.5 max: ~32,000 chars (8192 tokens)\n",
    "#\n",
    "# CHUNK combinations tested in this hyperparameter tuning:\n",
    "#   1)  800 /  120   (small chunks, fine-grained search)\n",
    "#   2) 2000 /  300\n",
    "#   3) 3000 /  500   <- baseline (current default) good\n",
    "#   4) 4500 /  600   OK, 30min\n",
    "#   5) 6000 /  800   Not bad but not good, 1hr\n",
    "#   6) 8000 / 1000   (large chunks, max context preservation) Not bad but not good, 1hr\n",
    "#\n",
    "# Round 1: Extreme comparison 800/120 vs 8000/1000 -> additional experiments with surviving range\n",
    "# Round 2: Compare final candidates with 3000/500 baseline to finalize\n",
    "CHUNK_SIZE = 3000  # Baseline: 3000 (change per experiment)\n",
    "\n",
    "# CHUNK_OVERLAP: Overlap between chunks (in characters)\n",
    "#    - Recommended range: 10-20% of CHUNK_SIZE (100~1000)\n",
    "#    - Ensures context continuity, prevents info loss at boundaries\n",
    "CHUNK_OVERLAP = 500  # Baseline: 500 (change per experiment)\n",
    "\n",
    "# TOP_K_DOCUMENTS: Number of chunks to retrieve\n",
    "#    - Recommended range: 3 ~ 6 (2 ~ 6)\n",
    "#    - Smaller: Faster, only top accurate results\n",
    "#    - Larger: Diverse perspectives, but more noise\n",
    "TOP_K_DOCUMENTS = 4  # Recommended: 4 (balanced)\n",
    "\n",
    "# LLM_TEMPERATURE: Answer creativity\n",
    "#    - 0.0: Deterministic, fact-based (suitable for Strict RAG)\n",
    "#    - 0.7+: Creative, diverse expressions\n",
    "LLM_TEMPERATURE = 0.2  # Recommended: 0.0 (fact-based)\n",
    "\n",
    "# ========================================\n",
    "# STEP 2: Strict RAG Settings\n",
    "# ========================================\n",
    "# SIGMOID_MIDPOINT: Center point of score distribution\n",
    "#    - Recommended range: 0.45, 0.48, 0.5, 0.52, 0.55, 0.58, 0.60\n",
    "#    - Higher: Lower scores become even lower (stricter)\n",
    "#    - Adjust after testing with new embedding model\n",
    "SIGMOID_MIDPOINT = 0.50  # Recommended: 0.50-0.55 (adjust after testing)\n",
    "\n",
    "# SIGMOID_STEEPNESS: Score separation strength\n",
    "#    - Recommended range: 8, 10, 12, 14, 16, 18, 20\n",
    "#    - Higher: Mid-range scores pushed to extremes\n",
    "SIGMOID_STEEPNESS = 18  # Recommended: 12-15\n",
    "\n",
    "# RELEVANCE_THRESHOLD: Answer eligibility threshold\n",
    "#    - Recommended range: 0.50 ~ 0.65\n",
    "#    - Returns 'No Answer' if score is below this\n",
    "#    - Adjust after testing with new embedding model\n",
    "RELEVANCE_THRESHOLD = 0.6\n",
    "\n",
    "# Test Questions\n",
    "# Various questions to test Relevance Score distribution\n",
    "# Expected distribution: High(0.8+) -> Medium(0.5-0.7) -> Low(0.2-0.4) -> Very Low(<0.2)\n",
    "TEST_QUERIES = [\n",
    "    # Case 1: High relevance, must-be-RAG\n",
    "    \"What are the key degradation mechanisms of a blue phosphorescent OLED?\",\n",
    "    \n",
    "    # Case 2: High but slightly lower than Case 1\n",
    "    \"How does exciton diffusion length affect charge separation efficiency in organic semiconductor devices?\",\n",
    "    \n",
    "    # Case 3: Display but non-OLED\n",
    "    \"What are the major challenges in mass transfer processes for MicroLED displays?\",  \n",
    "    \n",
    "    # Case 4: Non-display semiconductor device\n",
    "    \"How does doping concentration affect the electron mobility in silicon MOSFETs?\",\n",
    "    \n",
    "    # Case 5: Science but non-electronics\n",
    "    \"Describe the physical principles used to reduce aerodynamic drag in automotive design\",\n",
    "\n",
    "    # Case 6: Non-science, off-topic\n",
    "    \"Recommend a good hiking trail near Santa Clara for a weekend trip\",  \n",
    "]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Configuration Complete!\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Document Folder: {DOCS_FOLDER}\")\n",
    "print(f\"DB Path: {DB_PATH}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging, Monitoring & Error Handling Setup\n",
    "\n",
    "**Why add this before hyperparameter tuning?**\n",
    "\n",
    "When experimenting with various hyperparameters, you need:\n",
    "1. **Logging**: Record what's happening (for debugging)\n",
    "2. **Monitoring**: Track performance (response time, cost)\n",
    "3. **Error Handling**: Handle errors gracefully (API failures, timeouts)\n",
    "\n",
    "---\n",
    "\n",
    "### What We Will Track\n",
    "\n",
    "**Performance Metrics:**\n",
    "- Response time per query\n",
    "- Token usage and estimated cost\n",
    "- Mode distribution (RAG vs NO_ANSWER vs OFF_TOPIC)\n",
    "\n",
    "**Error Handling:**\n",
    "- LLM API errors (Rate limits, timeouts from OpenAI or local Mistral OpenAI-compatible servers)\n",
    "- Document loading errors (file not found, corrupted PDF)\n",
    "- Automatic retry for transient errors\n",
    "\n",
    "**Experiment Tracking:**\n",
    "- Auto-save results to CSV files\n",
    "- Compare different hyperparameter settings\n",
    "- Find optimal settings based on data\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Logging, Monitoring & Error Handling Setup\n",
    "# This cell sets up logging, cost tracking, error handling, and experiment tracking\n",
    "\n",
    "import logging\n",
    "import time\n",
    "import csv\n",
    "import os\n",
    "from datetime import datetime\n",
    "from functools import wraps\n",
    "import tiktoken\n",
    "\n",
    "# ========================================\n",
    "# üìù LOGGING SETUP\n",
    "# ========================================\n",
    "# Create logs directory if it doesn't exist\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "\n",
    "# Configure logging\n",
    "# Note: remove existing handlers before reconfiguring so the cell can be run multiple times safely\n",
    "# Remove all existing handlers so repeated cell runs behave reliably\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "    handler.close()  # Clean up handler resources\n",
    "\n",
    "# Then apply the base configuration\n",
    "log_filename = f'logs/AI_OLED_assistant_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log'\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_filename),\n",
    "        logging.StreamHandler()  # Also print to console\n",
    "    ],\n",
    "    force=True  # Python 3.8+: force reconfiguration to avoid duplicate handlers\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.info(f\"üìù Logging initialized: {log_filename}\")\n",
    "\n",
    "# ========================================\n",
    "# üí∞ COST TRACKING\n",
    "# ========================================\n",
    "class CostTracker:\n",
    "    \"\"\"\n",
    "    Track token usage and estimated API costs.\n",
    "    \n",
    "    OpenAI pricing (as of 2024):\n",
    "    - GPT-4o-mini: $0.150 per 1M input tokens, $0.600 per 1M output tokens\n",
    "    \"\"\"\n",
    "    \n",
    "    # Pricing per 1M tokens (USD)\n",
    "    PRICING = {\n",
    "        \"gpt-5\": {\"input\": 1.25, \"output\": 10.00},\n",
    "        \"gpt-5-mini\": {\"input\": 0.25, \"output\": 2.00},\n",
    "        \"gpt-4o-mini\": {\"input\": 0.15, \"output\": 0.60},\n",
    "        # Local LLM (Mistral, etc.) has zero cost, only track tokens\n",
    "        \"mistral-nemo\": {\"input\": 0.0, \"output\": 0.0},\n",
    "    }\n",
    "    \n",
    "    def __init__(self, model_name=\"gpt-4o-mini\"):\n",
    "        \"\"\"Initialize cost tracker for a specific model.\"\"\"\n",
    "        self.model_name = model_name\n",
    "        # Use safe default encoding for model names unknown to tiktoken\n",
    "        try:\n",
    "            self.encoding = tiktoken.encoding_for_model(model_name)\n",
    "        except Exception:\n",
    "            self.encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        self.total_input_tokens = 0\n",
    "        self.total_output_tokens = 0\n",
    "    \n",
    "    def count_tokens(self, text):\n",
    "        \"\"\"Count tokens in a text string.\"\"\"\n",
    "        return len(self.encoding.encode(text))\n",
    "    \n",
    "    def add_tokens(self, input_text, output_text):\n",
    "        \"\"\"\n",
    "        Add token counts for input and output.\n",
    "        \n",
    "        Args:\n",
    "            input_text: Input prompt text\n",
    "            output_text: Model response text\n",
    "        \"\"\"\n",
    "        input_tokens = self.count_tokens(input_text)\n",
    "        output_tokens = self.count_tokens(output_text)\n",
    "        \n",
    "        self.total_input_tokens += input_tokens\n",
    "        self.total_output_tokens += output_tokens\n",
    "        \n",
    "        return input_tokens, output_tokens\n",
    "    \n",
    "    def get_cost(self):\n",
    "        \"\"\"Calculate total cost in USD.\"\"\"\n",
    "        if self.model_name not in self.PRICING:\n",
    "            return 0.0  # Unknown model\n",
    "        \n",
    "        pricing = self.PRICING[self.model_name]\n",
    "        input_cost = (self.total_input_tokens / 1_000_000) * pricing[\"input\"]\n",
    "        output_cost = (self.total_output_tokens / 1_000_000) * pricing[\"output\"]\n",
    "        \n",
    "        return input_cost + output_cost\n",
    "    \n",
    "    def get_summary(self):\n",
    "        \"\"\"Get summary of token usage and cost.\"\"\"\n",
    "        return {\n",
    "            \"input_tokens\": self.total_input_tokens,\n",
    "            \"output_tokens\": self.total_output_tokens,\n",
    "            \"total_tokens\": self.total_input_tokens + self.total_output_tokens,\n",
    "            \"estimated_cost_usd\": self.get_cost()\n",
    "        }\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset token counters.\"\"\"\n",
    "        self.total_input_tokens = 0\n",
    "        self.total_output_tokens = 0\n",
    "\n",
    "# Initialize global cost tracker\n",
    "cost_tracker = CostTracker(model_name=LLM_MODEL)\n",
    "\n",
    "# ========================================\n",
    "# üõ°Ô∏è ERROR HANDLING UTILITIES\n",
    "# ========================================\n",
    "def retry_on_api_error(max_retries=2, delay=2):\n",
    "    \"\"\"\n",
    "    Decorator to retry function on API errors.\n",
    "    \n",
    "    Args:\n",
    "        max_retries: Maximum number of retry attempts\n",
    "        delay: Delay in seconds between retries\n",
    "    \n",
    "    Usage:\n",
    "        @retry_on_api_error(max_retries=2, delay=2)\n",
    "        def api_call():\n",
    "            # Your API call here\n",
    "            pass\n",
    "    \"\"\"\n",
    "    def decorator(func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            last_exception = None\n",
    "            \n",
    "            for attempt in range(max_retries + 1):\n",
    "                try:\n",
    "                    return func(*args, **kwargs)\n",
    "                except Exception as e:\n",
    "                    last_exception = e\n",
    "                    error_type = type(e).__name__\n",
    "                    \n",
    "                    # Log the error\n",
    "                    if attempt < max_retries:\n",
    "                        logger.warning(f\"‚ö†Ô∏è {error_type} on attempt {attempt + 1}/{max_retries + 1}: {str(e)}\")\n",
    "                        logger.info(f\"üîÑ Retrying in {delay} seconds...\")\n",
    "                        time.sleep(delay)\n",
    "                    else:\n",
    "                        logger.error(f\"‚ùå Failed after {max_retries + 1} attempts: {str(e)}\")\n",
    "            \n",
    "            # If all retries failed, raise the last exception\n",
    "            raise last_exception\n",
    "        \n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "def safe_file_load(file_path, loader_class):\n",
    "    \"\"\"\n",
    "    Safely load a file with error handling.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to file\n",
    "        loader_class: LangChain loader class (PyPDFLoader or Docx2txtLoader)\n",
    "    \n",
    "    Returns:\n",
    "        List of loaded documents, or empty list if failed\n",
    "    \"\"\"\n",
    "    try:\n",
    "        loader = loader_class(file_path)\n",
    "        documents = loader.load()\n",
    "        logger.info(f\"‚úÖ Loaded: {os.path.basename(file_path)} ({len(documents)} pages/sections)\")\n",
    "        return documents\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"‚ùå File not found: {file_path}\")\n",
    "        return []\n",
    "    except PermissionError:\n",
    "        logger.error(f\"‚ùå Permission denied: {file_path}\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Error loading {os.path.basename(file_path)}: {type(e).__name__} - {str(e)}\")\n",
    "        return []\n",
    "\n",
    "# ========================================\n",
    "# üìä EXPERIMENT TRACKING\n",
    "# ========================================\n",
    "class ExperimentTracker:\n",
    "    \"\"\"Track and save hyperparameter experiment results.\"\"\"\n",
    "    \n",
    "    def __init__(self, csv_filename=\"hyperparameter_experiments.csv\"):\n",
    "        \"\"\"Initialize experiment tracker.\"\"\"\n",
    "        self.csv_filename = csv_filename\n",
    "        self.current_experiment = {}\n",
    "        \n",
    "        # Create CSV file with headers if it doesn't exist\n",
    "        if not os.path.exists(csv_filename):\n",
    "            with open(csv_filename, 'w', newline='') as f:\n",
    "                # CSV headers for Strict RAG (no LLM baseline comparison)\n",
    "                writer = csv.DictWriter(f, fieldnames=[\n",
    "                    'timestamp',  # 1\n",
    "\n",
    "                    # RAG Quality (Step 1)\n",
    "                    'chunk_size', 'chunk_overlap', 'top_k', 'temperature',  # 2-5\n",
    "\n",
    "                    # System Architecture (Step 2) - Used for Strict RAG only\n",
    "                    'relevance_threshold', 'sigmoid_midpoint', 'sigmoid_steepness',  # 6-8\n",
    "\n",
    "                    # Performance\n",
    "                    'avg_response_time_sec', 'total_tokens', 'estimated_cost_usd',  # 9-11\n",
    "\n",
    "                    # Mode Distribution (Strict RAG: RAG, NO_ANSWER_IN_DOCS, OFF_TOPIC only)\n",
    "                    'mode_rag_count', 'mode_no_answer_count', 'mode_off_topic_count', 'mode_error_count',  # 12-15\n",
    "\n",
    "                    # Score Metrics (RAG only, no LLM baseline)\n",
    "                    'avg_relevance_score', 'avg_rag_score',  # 16-17\n",
    "                    'avg_rag_specificity', 'avg_rag_relevance', 'avg_rag_factuality',  # 18-20\n",
    "\n",
    "                    # Relevance separation metrics (Q1‚ÄìQ2 vs Q5‚ÄìQ6)\n",
    "                    'rel_mean_pos', 'rel_mean_neg', 'rel_gap', 'rel_margin',  # 21-24\n",
    "\n",
    "                    'notes'  # 25\n",
    "                ])\n",
    "                writer.writeheader()\n",
    "    \n",
    "    def start_experiment(self, config):\n",
    "        \"\"\"\n",
    "        Start tracking a new experiment.\n",
    "        \n",
    "        Args:\n",
    "            config: Dictionary with hyperparameter settings\n",
    "        \"\"\"\n",
    "        # Strict RAG modes: RAG, NO_ANSWER_IN_DOCS, OFF_TOPIC only\n",
    "        self.current_experiment = {\n",
    "            'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            **config,\n",
    "            'response_times': [],\n",
    "            'mode_counts': {\n",
    "                'RAG': 0, \n",
    "                'NO_ANSWER_IN_DOCS': 0,\n",
    "                'OFF_TOPIC': 0,\n",
    "                'ERROR': 0\n",
    "            }\n",
    "        }\n",
    "        logger.info(f\"üß™ Starting experiment with config: {config}\")\n",
    "    \n",
    "    def log_query(self, response_time, mode):\n",
    "        \"\"\"Log a single query result.\"\"\"\n",
    "        self.current_experiment['response_times'].append(response_time)\n",
    "        self.current_experiment['mode_counts'][mode] += 1\n",
    "    \n",
    "    def save_experiment(self, \n",
    "                       avg_relevance_score=None, avg_rag_score=None,\n",
    "                       avg_rag_specificity=None, avg_rag_relevance=None, avg_rag_factuality=None,\n",
    "                       rel_mean_pos=None, rel_mean_neg=None, rel_gap=None, rel_margin=None,\n",
    "                       notes=\"\"):\n",
    "        \"\"\"Save Strict RAG experiment results to CSV.\n",
    "\n",
    "        Args\n",
    "        ----\n",
    "        avg_relevance_score : float | None\n",
    "            Average relevance score (0-1)\n",
    "        avg_rag_score : float | None\n",
    "            Average RAG answer quality (1-10)\n",
    "        avg_rag_specificity : float | None\n",
    "            Average RAG specificity score (1-10)\n",
    "        avg_rag_relevance : float | None\n",
    "            Average RAG relevance score (1-10)\n",
    "        avg_rag_factuality : float | None\n",
    "            Average RAG factuality score (1-10)\n",
    "        rel_mean_pos : float | None\n",
    "            Q1-Q2 (OLED core) relevance average\n",
    "        rel_mean_neg : float | None\n",
    "            Q5-Q6 (OFF-TOPIC) relevance average\n",
    "        rel_gap : float | None\n",
    "            rel_mean_pos - rel_mean_neg\n",
    "        rel_margin : float | None\n",
    "            min(Q1-Q2) - max(Q5-Q6)\n",
    "        notes : str\n",
    "            Additional notes for the experiment\n",
    "        \"\"\"\n",
    "        cost_summary = cost_tracker.get_summary()\n",
    "        \n",
    "        # Calculate averages\n",
    "        avg_response_time = sum(self.current_experiment['response_times']) / len(self.current_experiment['response_times']) if self.current_experiment['response_times'] else 0\n",
    "        \n",
    "        # Row data for Strict RAG (matches CSV header order)\n",
    "        row = {\n",
    "            'timestamp': self.current_experiment['timestamp'],\n",
    "\n",
    "            # RAG Quality (Step 1)\n",
    "            'chunk_size': self.current_experiment.get('chunk_size'),\n",
    "            'chunk_overlap': self.current_experiment.get('chunk_overlap'),\n",
    "            'top_k': self.current_experiment.get('top_k'),\n",
    "            'temperature': self.current_experiment.get('temperature'),\n",
    "\n",
    "            # System Architecture (Step 2) - Used for Strict RAG only\n",
    "            'relevance_threshold': self.current_experiment.get('relevance_threshold'),\n",
    "            'sigmoid_midpoint': self.current_experiment.get('sigmoid_midpoint'),\n",
    "            'sigmoid_steepness': self.current_experiment.get('sigmoid_steepness'),\n",
    "            \n",
    "            # Performance\n",
    "            'avg_response_time_sec': f\"{avg_response_time:.2f}\",\n",
    "            'total_tokens': cost_summary['total_tokens'],\n",
    "            'estimated_cost_usd': f\"${cost_summary['estimated_cost_usd']:.4f}\",\n",
    "\n",
    "            # Mode Distribution (Strict RAG: RAG, NO_ANSWER_IN_DOCS, OFF_TOPIC only)\n",
    "            'mode_rag_count': self.current_experiment['mode_counts']['RAG'],\n",
    "            'mode_no_answer_count': self.current_experiment['mode_counts']['NO_ANSWER_IN_DOCS'],\n",
    "            'mode_off_topic_count': self.current_experiment['mode_counts']['OFF_TOPIC'],\n",
    "            'mode_error_count': self.current_experiment['mode_counts']['ERROR'],\n",
    "            \n",
    "            # Score Metrics (RAG only, no LLM baseline)\n",
    "            'avg_relevance_score': f\"{avg_relevance_score:.3f}\" if avg_relevance_score is not None else \"N/A\",\n",
    "            'avg_rag_score': f\"{avg_rag_score:.2f}\" if avg_rag_score is not None else \"N/A\",\n",
    "            'avg_rag_specificity': f\"{avg_rag_specificity:.2f}\" if avg_rag_specificity is not None else \"N/A\",\n",
    "            'avg_rag_relevance': f\"{avg_rag_relevance:.2f}\" if avg_rag_relevance is not None else \"N/A\",\n",
    "            'avg_rag_factuality': f\"{avg_rag_factuality:.2f}\" if avg_rag_factuality is not None else \"N/A\",\n",
    "\n",
    "            # Relevance separation metrics (Q1‚ÄìQ2 vs Q5‚ÄìQ6)\n",
    "            'rel_mean_pos': f\"{rel_mean_pos:.3f}\" if rel_mean_pos is not None else \"N/A\",\n",
    "            'rel_mean_neg': f\"{rel_mean_neg:.3f}\" if rel_mean_neg is not None else \"N/A\",\n",
    "            'rel_gap': f\"{rel_gap:.3f}\" if rel_gap is not None else \"N/A\",\n",
    "            'rel_margin': f\"{rel_margin:.3f}\" if rel_margin is not None else \"N/A\",\n",
    "\n",
    "            'notes': notes\n",
    "        }\n",
    "        \n",
    "        # Append to CSV\n",
    "        with open(self.csv_filename, 'a', newline='') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=row.keys())\n",
    "            writer.writerow(row)\n",
    "        \n",
    "        logger.info(f\"üíæ Experiment saved to {self.csv_filename}\")\n",
    "        logger.info(f\"üìä Summary: Avg time={avg_response_time:.2f}s, Cost=${cost_summary['estimated_cost_usd']:.4f}\")\n",
    "\n",
    "# Initialize global experiment tracker\n",
    "experiment_tracker = ExperimentTracker()\n",
    "\n",
    "print(\"‚úÖ Logging, Monitoring & Error Handling configured successfully!\")\n",
    "print(f\"üìù Logs will be saved to: logs/\")\n",
    "print(f\"üìä Experiment results will be saved to: hyperparameter_experiments.csv\")\n",
    "print(f\"üí∞ Cost tracking enabled for model: {LLM_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging, Monitoring & Error Handling Usage\n",
    "\n",
    "**Automatic Logging:**\n",
    "- All operations are logged to `logs/AI_OLED_assistant_[timestamp].log`\n",
    "- Includes timestamps, error messages, and performance metrics\n",
    "- Automatically monitored in the background\n",
    "\n",
    "**Cost Tracking:**\n",
    "- Token usage for each query is automatically calculated\n",
    "- Estimated API cost is computed in real-time\n",
    "- Summary is printed after each experiment\n",
    "\n",
    "**Error Handling:**\n",
    "- Automatic retry on API failure (up to 2 times)\n",
    "- File loading errors are logged\n",
    "- User-friendly error messages are returned\n",
    "\n",
    "**Experiment Tracking:**\n",
    "- All hyperparameter experiments are saved to `hyperparameter_experiments.csv`\n",
    "- Includes response time, token usage, cost, mode distribution, etc.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Loading Setup\n",
    "\n",
    "**What This Section Does:**\n",
    "Configuration section for document loading. Actual loading is performed in the next section.\n",
    "\n",
    "**Environment:**\n",
    "- Designed for local execution\n",
    "- Reads OLED technical documents directly from `data/` folder\n",
    "\n",
    "## Document Loading and Chunking\n",
    "\n",
    "**What This Section Does:**\n",
    "This is where OLED technical documents are actually loaded and processed.\n",
    "\n",
    "**Step-by-step Process:**\n",
    "\n",
    "1. **File Discovery** (`glob.glob` with `recursive=True`)\n",
    "   - Scans for PDF/DOCX files in `data/` folder and **all subfolders**\n",
    "   - Documents organized in folders are automatically recognized\n",
    "\n",
    "2. **Document Loading** (`PyPDFLoader`, `Docx2txtLoader`)\n",
    "   - PDF ‚Üí text conversion (one page = one document)\n",
    "   - DOCX ‚Üí text conversion (one section = one document)\n",
    "   - `safe_file_load()` wrapper prevents corrupted files from crashing the notebook\n",
    "\n",
    "3. **Text Chunking** (`RecursiveCharacterTextSplitter`)\n",
    "   - Splits long documents into smaller chunks\n",
    "   - Each chunk is `CHUNK_SIZE` characters (e.g., current setting: 3000)\n",
    "   - `CHUNK_OVERLAP` characters overlap between chunks (e.g., current setting: 500)\n",
    "   - Why overlap? Prevents context loss when sentences are cut at chunk boundaries\n",
    "\n",
    "**Why We Chunk Documents:**\n",
    "- **LLM token limit**: Models have maximum input size (e.g., 128K tokens)\n",
    "- **Better retrieval**: Smaller chunks match queries more accurately\n",
    "- **Context preservation**: Overlap prevents important info loss at boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Load and Chunk Documents\n",
    "# This cell loads all PDF and DOCX files from the docs folder and splits them into chunks\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# ========================================\n",
    "# 0. Fast path: If ChromaDB already exists, skip loading & chunking\n",
    "#    - If DB exists, skip PDF loading + chunking, load existing DB in Cell 12\n",
    "# ========================================\n",
    "if os.path.exists(DB_PATH) and os.listdir(DB_PATH):\n",
    "    print(f\"üìÇ Existing ChromaDB found at {DB_PATH}\")\n",
    "    print(\"‚è≠Ô∏è Skipping document loading & chunking (using existing DB only).\")\n",
    "    logger.info(f\"Skip loading/chunking because DB already exists at {DB_PATH}\")\n",
    "\n",
    "else:\n",
    "    # ========================================\n",
    "    # STEP 1: Find all PDF and DOCX files\n",
    "    # ========================================\n",
    "    logger.info(f\"üìÇ Scanning folder: {DOCS_FOLDER}\")\n",
    "\n",
    "    # Check if docs folder exists before proceeding\n",
    "    if not os.path.exists(DOCS_FOLDER):\n",
    "        logger.error(f\"‚ùå Docs folder not found: {DOCS_FOLDER}\")\n",
    "        raise FileNotFoundError(f\"Documents folder '{DOCS_FOLDER}' does not exist\")\n",
    "\n",
    "    # Use glob to find all PDF and DOCX files in the folder (including subfolders)\n",
    "    # recursive=True and ** pattern enables searching in subdirectories\n",
    "    pdf_files = glob.glob(os.path.join(DOCS_FOLDER, \"**/*.pdf\"), recursive=True)\n",
    "    docx_files = glob.glob(os.path.join(DOCS_FOLDER, \"**/*.docx\"), recursive=True)\n",
    "\n",
    "    print(f\"üìÇ Found {len(pdf_files)} PDF files and {len(docx_files)} DOCX files in {DOCS_FOLDER}/ (including subfolders)\")\n",
    "    logger.info(f\"Found {len(pdf_files)} PDFs and {len(docx_files)} DOCX files\")\n",
    "\n",
    "    # Check if any files found - warn if folder is empty\n",
    "    if len(pdf_files) == 0 and len(docx_files) == 0:\n",
    "        logger.warning(f\"‚ö†Ô∏è No PDF or DOCX files found in {DOCS_FOLDER}/\")\n",
    "        print(f\"‚ö†Ô∏è WARNING: No documents found. Please add PDF or DOCX files to {DOCS_FOLDER}/\")\n",
    "\n",
    "    # ========================================\n",
    "    # STEP 2: Load all documents into memory\n",
    "    # ========================================\n",
    "    all_documents = []  # This will store all loaded document pages/sections\n",
    "\n",
    "    # Load PDF files using PyPDFLoader (one page = one document)\n",
    "    # safe_file_load() wraps the loader with error handling (won't crash on corrupt files)\n",
    "    print(f\"\\nüìÑ Loading PDF files...\")\n",
    "    for pdf_file in pdf_files:\n",
    "        # safe_file_load() handles errors gracefully - returns [] on failure\n",
    "        documents = safe_file_load(pdf_file, PyPDFLoader)\n",
    "        all_documents.extend(documents)  # Add all pages from this PDF to our collection\n",
    "\n",
    "    # Load DOCX files using Docx2txtLoader (one section = one document)\n",
    "    print(f\"\\nüìÑ Loading DOCX files...\")\n",
    "    for docx_file in docx_files:\n",
    "        documents = safe_file_load(docx_file, Docx2txtLoader)\n",
    "        all_documents.extend(documents)\n",
    "\n",
    "    # ========================================\n",
    "    # STEP 3: Validate that documents were loaded\n",
    "    # ========================================\n",
    "    if len(all_documents) == 0:\n",
    "        logger.error(\"‚ùå No documents were successfully loaded\")\n",
    "        raise ValueError(\"Failed to load any documents. Please check file formats and permissions.\")\n",
    "\n",
    "    print(f\"\\n‚úÖ Total documents loaded: {len(all_documents)}\")\n",
    "    logger.info(f\"Successfully loaded {len(all_documents)} document sections\")\n",
    "\n",
    "    # ========================================\n",
    "    # STEP 4: Split documents into chunks\n",
    "    # ========================================\n",
    "    # RecursiveCharacterTextSplitter splits text intelligently:\n",
    "    # - Tries to split at paragraph breaks first, then sentences, then words\n",
    "    # - Ensures chunks are approximately CHUNK_SIZE characters\n",
    "    # - Adds CHUNK_OVERLAP characters of overlap between chunks to preserve context\n",
    "    try:\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=CHUNK_SIZE,      # Target size: CHUNK_SIZE characters per chunk\n",
    "            chunk_overlap=CHUNK_OVERLAP  # Overlap: CHUNK_OVERLAP characters between chunks\n",
    "        )\n",
    "        # Split all documents into chunks\n",
    "        docs = text_splitter.split_documents(all_documents)\n",
    "        # What split_documents() does:\n",
    "            # 1. Reads each Document's page_content.\n",
    "            # 2. Splits text into ~CHUNK_SIZE-character chunks (paragraph/sentence boundaries preferred).\n",
    "            # 3. Creates a new Document object for each chunk.\n",
    "            # 4. Preserves original metadata (source, page, etc.).\n",
    "        # Result: docs = [chunk_1, chunk_2, chunk_3, ...]\n",
    "        # Each chunk is a Document object\n",
    "        print(f\"üîπ Total chunks created: {len(docs)}\")\n",
    "        logger.info(f\"Created {len(docs)} text chunks (size={CHUNK_SIZE}, overlap={CHUNK_OVERLAP})\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Error splitting documents: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding and Vector Store Creation\n",
    "\n",
    "**What This Section Does:**\n",
    "Converts all document chunks into numerical vectors (embeddings) and builds a searchable index.\n",
    "\n",
    "**Why We Need Embeddings:**\n",
    "- Raw text cannot be directly compared by computers\n",
    "- Embeddings convert text into numerical vectors that capture **semantic meaning**\n",
    "- Similar text ‚Üí similar vectors ‚Üí vector comparison enables relevant document retrieval\n",
    "\n",
    "**Process:**\n",
    "\n",
    "1. **Embedding Model Initialization**\n",
    "   - Using **BAAI/bge-m3** (multilingual, multi-task embedding, medium size)\n",
    "   - Based on sentence-transformers, easily usable with `HuggingFaceEmbeddings`\n",
    "   - Why? Lighter than gte-large, runs at realistic speeds on local Mac(MPS) while maintaining excellent search quality\n",
    "\n",
    "2. **Vector Embedding Generation**\n",
    "   - Each document chunk is converted to a vector (high-dimensional number array)\n",
    "   - These vectors represent the **semantic meaning** of the text\n",
    "\n",
    "3. **Vector Store Construction** (`ChromaDB`)\n",
    "   - Stores all embeddings for fast retrieval\n",
    "   - Builds an index enabling similarity search\n",
    "   - Supports local persistent storage\n",
    "\n",
    "**Next Steps:**\n",
    "- When you ask a question, the query is also converted to an embedding\n",
    "- The system compares the query embedding with all document embeddings\n",
    "- Returns the most similar documents (based on cosine similarity scores)\n",
    "\n",
    "**Key Advantages:**\n",
    "- Goes beyond simple keyword matching (understands \"OLED\" and \"organic light-emitting diode\" are related)\n",
    "- Captures semantic relationships (e.g., \"phosphorescent materials\" vs \"blue phosphorescence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Vector Store Construction (ChromaDB with Persistence)\n",
    "# This cell embeds documents and saves to ChromaDB, or loads existing DB if present.\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# ========================================\n",
    "# STEP 1: Embedding Model Initialization\n",
    "# ========================================\n",
    "# BAAI/bge-m3: Medium-sized multilingual/multi-task embedding model\n",
    "#    - sentence-transformers based -> easy to use with HuggingFaceEmbeddings\n",
    "#    - Lighter than gte-large, realistic speed on Mac(MPS)\n",
    "EMBEDDING_MODEL = \"BAAI/bge-m3\"\n",
    "\n",
    "try:\n",
    "    print(\"Loading embedding model...\")\n",
    "    print(f\"Model: {EMBEDDING_MODEL}\")\n",
    "    \n",
    "    import torch\n",
    "    \n",
    "    # Device configuration\n",
    "    if torch.cuda.is_available():\n",
    "        device = 'cuda'\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = 'mps'\n",
    "    else:\n",
    "        device = 'cpu'\n",
    "    print(f\"Device: {device.upper()}\")\n",
    "    \n",
    "    # Load BGE-m3 embedding model\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=EMBEDDING_MODEL,\n",
    "        model_kwargs={\n",
    "            'device': device,\n",
    "        },\n",
    "        encode_kwargs={\n",
    "            'normalize_embeddings': True,  # Normalize for cosine similarity\n",
    "            'batch_size': 16,              # Batch size (adjustable between 8~32)\n",
    "        }\n",
    "    )\n",
    "    print(f\"Embedding model loaded! ({EMBEDDING_MODEL})\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Failed to load embedding model: {e}\")\n",
    "    raise\n",
    "\n",
    "# ========================================\n",
    "# STEP 2: Load or Create ChromaDB (Check -> Exist?)\n",
    "# ========================================\n",
    "def get_vectorstore():\n",
    "    # Check if DB folder exists and is not empty\n",
    "    if os.path.exists(DB_PATH) and os.listdir(DB_PATH):\n",
    "        print(f\"\\nExisting ChromaDB found: {DB_PATH}\")\n",
    "        print(\"Loading existing DB... (skipping embedding)\")\n",
    "        \n",
    "        vectorstore = Chroma(\n",
    "            persist_directory=DB_PATH,\n",
    "            embedding_function=embeddings\n",
    "        )\n",
    "        print(\"Existing DB loaded!\")\n",
    "        return vectorstore\n",
    "    else:\n",
    "        print(f\"\\nNo existing DB found. Embedding documents from scratch.\")\n",
    "        print(f\"Embedding {len(docs)} chunks... (this may take a while)\")\n",
    "        \n",
    "        # Remove empty folder if exists before creating new DB\n",
    "        if os.path.exists(DB_PATH):\n",
    "            shutil.rmtree(DB_PATH)\n",
    "            \n",
    "        vectorstore = Chroma.from_documents(\n",
    "            documents=docs,\n",
    "            embedding=embeddings,\n",
    "            persist_directory=DB_PATH\n",
    "        )\n",
    "        # Chroma auto-persists, but explicitly save\n",
    "        # vectorstore.persist() # Auto-saved in newer versions\n",
    "        print(f\"New DB created and saved: {DB_PATH}\")\n",
    "        return vectorstore\n",
    "\n",
    "try:\n",
    "    vectorstore = get_vectorstore()\n",
    "    # Verify data\n",
    "    count = vectorstore._collection.count()\n",
    "    print(f\"Document chunks stored in DB: {count}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error processing vector store: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strict RAG System\n",
    "\n",
    "**What This Is:**\n",
    "The **core engine** of the OLED Assistant. A Strict RAG system that uses documents as the **primary source** and only allows OLED/physics knowledge as supplementary when context is partial.\n",
    "\n",
    "**Problem It Solves:**\n",
    "- If the answer is in documents -> Answer with RAG using documents first (limited LLM knowledge as backup if needed)\n",
    "- If documents have no relevant content -> Return \"No Answer\" or \"Information not found in documents\"\n",
    "- Questions clearly unrelated to OLED -> Auto-reject (OFF_TOPIC)\n",
    "\n",
    "**How the Decision Process Works:**\n",
    "\n",
    "1. **Relevance Score Calculation (0.0 - 1.0)**\n",
    "   - Convert query to embedding\n",
    "   - Compare with document embeddings\n",
    "   - **Sigmoid transformation** to amplify separation:\n",
    "     - OLED-related queries -> boosted to 0.80-0.99 range\n",
    "     - Unrelated queries -> pushed to 0.01-0.20 range\n",
    "   - Post-sigmoid average similarity = **Relevance Score**\n",
    "\n",
    "2. **Three-Tier Decision System:**\n",
    "\n",
    "   **TIER 1: RAG Mode** (Relevance >= `RELEVANCE_THRESHOLD`)\n",
    "   - Documents are highly relevant\n",
    "   - Use RAG directly (documents + LLM)\n",
    "   - Result: Green answer\n",
    "\n",
    "   **TIER 2: No Information in Documents** (RAG returns \"no info\")\n",
    "   - Documents are relevant but don't have the specific answer\n",
    "   - Return \"Information not found\"\n",
    "   - Result: Orange (missing information)\n",
    "\n",
    "   **TIER 3: Off-Topic Auto-Rejection** (Relevance < `RELEVANCE_THRESHOLD`)\n",
    "   - Clearly unrelated to OLED displays\n",
    "   - Immediate rejection (no LLM call -> cost savings)\n",
    "   - Result: Red (auto-rejected)\n",
    "\n",
    "---\n",
    "\n",
    "### Understanding the Score System\n",
    "\n",
    "**1. Relevance Score (0.0 - 1.0)**\n",
    "- **Purpose**: \"Is the document relevant to this query?\"\n",
    "- **Calculation:**\n",
    "  1. Query -> embedding vector\n",
    "  2. Compare with top-K document embeddings (cosine similarity)\n",
    "  3. Calculate average similarity\n",
    "  4. Apply sigmoid transformation (amplify separation)\n",
    "- **Usage**: Determines which mode to use\n",
    "\n",
    "**2. RAG Quality Score (1-10)**\n",
    "- **Purpose**: \"How good is the RAG answer?\"\n",
    "- **Calculation:** LLM evaluates with 3 metrics, then averages:\n",
    "  - **Specificity** (1-10): How specific is the answer?\n",
    "  - **Relevance** (1-10): Does it actually answer the question?\n",
    "  - **Factuality** (1-10): Does it contain verifiable facts and data?\n",
    "- **Usage**: Evaluate RAG system performance\n",
    "\n",
    "**Key Distinction:**\n",
    "- **Relevance Score** = Quality of **document matching** (input quality)\n",
    "- **RAG Score** = Quality of **the answer itself** (output quality)\n",
    "- They are **independent** - high relevance can still produce low-quality answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Strict RAG Advisor Implementation\n",
    "# This cell implements a Strict RAG system that answers only based on document content.\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "\n",
    "def create_llm(provider: str, model_name: str, temperature: float):\n",
    "    \"\"\"LLM factory function.\n",
    "    \n",
    "    - provider=\"openai\"  -> OpenAI official endpoint (e.g., gpt-4o-mini)\n",
    "    - provider=\"llama_local\" -> OpenAI-compatible local/internal endpoint (e.g., Ollama, vLLM, internal LLM server)\n",
    "    \n",
    "    By separating this way, you can easily switch between OpenAI models and local Mistral/Llama/Gemma\n",
    "    by just changing the provider / model_name.\n",
    "    \"\"\"\n",
    "    if provider == \"openai\":\n",
    "        # Current setting: OpenAI API (e.g., gpt-4o-mini)\n",
    "        return ChatOpenAI(model=model_name, temperature=temperature)\n",
    "    elif provider == \"llama_local\":\n",
    "        # Local/internal OpenAI-compatible server (e.g., Ollama, vLLM)\n",
    "        # Default is Ollama OpenAI endpoint: http://localhost:11434/v1\n",
    "        base_url = os.getenv(\"LOCAL_LLM_BASE_URL\", os.getenv(\"LLAMA_BASE_URL\", \"http://localhost:11434/v1\"))\n",
    "        api_key = os.getenv(\"LOCAL_LLM_API_KEY\", os.getenv(\"LLAMA_API_KEY\", \"ollama\"))  # Ollama doesn't check tokens\n",
    "        return ChatOpenAI(\n",
    "            model=model_name,\n",
    "            temperature=temperature,\n",
    "            base_url=base_url,\n",
    "            api_key=api_key,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown LLM provider: {provider}\")\n",
    "\n",
    "\n",
    "class StrictRAGAdvisor:\n",
    "    \"\"\"Strict RAG System: Returns 'No Answer' if documents don't have the answer or question is off-topic.\"\"\"\n",
    "    \n",
    "    def __init__(self, vectorstore, llm_provider, llm_model, relevance_threshold, top_k, temperature, sigmoid_midpoint, sigmoid_steepness):\n",
    "        self.vectorstore = vectorstore\n",
    "        self.relevance_threshold = relevance_threshold\n",
    "        self.top_k = top_k\n",
    "        self.sigmoid_midpoint = sigmoid_midpoint\n",
    "        self.sigmoid_steepness = sigmoid_steepness\n",
    "        \n",
    "        # Create LLM here -> easily switch by changing provider later\n",
    "        self.llm = create_llm(provider=llm_provider, model_name=llm_model, temperature=temperature)\n",
    "        \n",
    "        # Strict RAG prompt (not complete block, use documents first but allow supplementary knowledge)\n",
    "        rag_prompt_template = \"\"\"You are an OLED Display Technical Assistant.\n",
    "Answer the question using the provided context documents as your PRIMARY source.\n",
    "\n",
    "RULES:\n",
    "1. Always read the Context carefully and base your answer as much as possible on the Context.\n",
    "2. If the Context contains partial but relevant information, you MAY use your own OLED/physics knowledge to fill in missing logical steps.\n",
    "3. ONLY when the Context is clearly irrelevant or provides almost no signal, say: \"Information not found in the provided OLED documents.\"\n",
    "4. Never contradict the facts given in the Context.\n",
    "5. Do NOT hallucinate specific numbers, experimental conditions, or paper titles that are not supported by the Context.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        RAG_PROMPT = PromptTemplate(\n",
    "            template = rag_prompt_template,\n",
    "            input_variables = [\"context\", \"question\"]\n",
    "        )\n",
    "        \n",
    "        self.rag_chain = RetrievalQA.from_chain_type(\n",
    "            llm = self.llm,\n",
    "            chain_type = \"stuff\",\n",
    "            retriever = vectorstore.as_retriever(search_kwargs={\"k\": top_k}),\n",
    "            chain_type_kwargs = {\"prompt\": RAG_PROMPT}\n",
    "        )\n",
    "        \n",
    "    def get_relevance_score(self, query):\n",
    "        # ChromaDB returns distance, need to convert to similarity\n",
    "        # Chroma L2 distance: lower is better (0 = identical)\n",
    "        # Similarity = 1 - distance (assuming normalized embeddings)\n",
    "        # Note: LangChain wrapper's similarity_search_with_score behavior needs verification\n",
    "        # LangChain Chroma's similarity_search_with_score returns L2 distance by default\n",
    "        \n",
    "        docs_with_scores = self.vectorstore.similarity_search_with_score(query, k=self.top_k)\n",
    "        if not docs_with_scores:\n",
    "            return 0.0\n",
    "        \n",
    "        # Score conversion (L2 distance -> Similarity)\n",
    "        # Distance 0 means similarity 1, distance >= 1 means low similarity\n",
    "        # Safe formula: similarity = 1.0 / (1.0 + distance)\n",
    "        scores = []\n",
    "        for doc, score in docs_with_scores:\n",
    "             # Chroma returns distance by default. Lower is better.\n",
    "             # Convert to 0-1 similarity score\n",
    "             similarity = 1.0 / (1.0 + score)\n",
    "             scores.append(similarity)\n",
    "        \n",
    "        avg_score = sum(scores) / len(scores)\n",
    "        \n",
    "        # Sigmoid transformation to spread score distribution\n",
    "        sigmoid_score = 1 / (1 + math.exp(-self.sigmoid_steepness * (avg_score - self.sigmoid_midpoint)))\n",
    "        return sigmoid_score\n",
    "    \n",
    "    def query(self, question):\n",
    "        # Strict RAG Logic\n",
    "        relevance_score = self.get_relevance_score(question)\n",
    "        \n",
    "        result = {\n",
    "            \"answer\": None, \n",
    "            \"mode\": None, \n",
    "            \"relevance_score\": relevance_score,\n",
    "            \"retrieved_docs\": []\n",
    "        }\n",
    "        \n",
    "        # Threshold check (reject immediately if document similarity is low)\n",
    "        if relevance_score >= self.relevance_threshold:\n",
    "            result[\"mode\"] = \"RAG\"\n",
    "            result[\"retrieved_docs\"] = self.vectorstore.similarity_search(question, k=self.top_k)\n",
    "            # Run RAG Chain (prompt instructs to say 'no info' if not found)\n",
    "            rag_response = self.rag_chain.invoke({\"query\": question})[\"result\"]\n",
    "            result[\"answer\"] = rag_response\n",
    "            \n",
    "            # Secondary check if LLM returned 'no info'\n",
    "            if \"Information not found\" in rag_response or \"provided context\" in rag_response and \"does not contain\" in rag_response:\n",
    "                 result[\"mode\"] = \"NO_ANSWER_IN_DOCS\"\n",
    "                 result[\"answer\"] = \"No Answer: Information not found in RAG documents.\"\n",
    "        else:\n",
    "            result[\"mode\"] = \"OFF_TOPIC\"\n",
    "            result[\"answer\"] = \"No Answer: This question is not related to OLED display or no relevant documents found.\"\n",
    "            logger.info(f\"üö´ Low relevance rejection: {relevance_score:.3f}\")\n",
    "            \n",
    "        return result\n",
    "\n",
    "# Initialize Strict Advisor\n",
    "advisor = StrictRAGAdvisor(\n",
    "    vectorstore = vectorstore,\n",
    "    llm_provider = LLM_PROVIDER,  # Can change to \"llama_local\" later\n",
    "    llm_model = LLM_MODEL,\n",
    "    relevance_threshold = RELEVANCE_THRESHOLD,\n",
    "    top_k = TOP_K_DOCUMENTS,\n",
    "    temperature = LLM_TEMPERATURE,\n",
    "    sigmoid_midpoint = SIGMOID_MIDPOINT,\n",
    "    sigmoid_steepness = SIGMOID_STEEPNESS\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Strict RAG Advisor (OLED) initialized successfully!\")\n",
    "print(f\"üìä Strict Threshold = {RELEVANCE_THRESHOLD}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhanced Query Function with Monitoring\n",
    "\n",
    "**What This Section Does:**\n",
    "Wraps the advisor's `query` method with automatic monitoring, logging, and error handling.\n",
    "\n",
    "**Why It's Needed:**\n",
    "- Track performance metrics (response time, cost, token usage)\n",
    "- Handle errors gracefully (API failures, timeouts)\n",
    "- Log experiment data for hyperparameter tuning\n",
    "- Systematically compare different settings\n",
    "\n",
    "**What Gets Tracked:**\n",
    "- **Response time**: Time taken for each query (seconds)\n",
    "- **Token usage & cost**: Input/output tokens and estimated API cost\n",
    "- **Mode distribution**: Query count per mode (RAG, NO_ANSWER, OFF_TOPIC, etc.)\n",
    "- **Errors & retries**: Automatic retry on transient failures (up to 2 times)\n",
    "\n",
    "**How It Works:**\n",
    "- `monitored_query()` function wraps `advisor.query()`\n",
    "- All metrics are automatically logged to the experiment tracker\n",
    "- Returns same structure as `advisor.query()` + additional metadata\n",
    "- `ask()` helper provides one-line interface for quick queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16: Enhanced Query Function with Monitoring\n",
    "# This cell creates a wrapper function that adds monitoring, error handling, and cost tracking\n",
    "\n",
    "def monitored_query(advisor, question, track_cost=True, log_to_experiment=True):\n",
    "    \"\"\"\n",
    "    Enhanced query function with automatic monitoring and error handling.\n",
    "    \n",
    "    What This Function Does:\n",
    "    1. Wraps advisor.query() with timing, error handling, and retry logic\n",
    "    2. Tracks token usage and calculates estimated API cost\n",
    "    3. Logs query results to experiment tracker for analysis\n",
    "    4. Returns same format as advisor.query() + additional metadata\n",
    "    \n",
    "    Args:\n",
    "        advisor: StrictRAGAdvisor instance (the main advisor object for OLED RAG)\n",
    "        question: User's question text\n",
    "        track_cost: Whether to track token usage and cost (default: True)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Result dictionary with answer, metadata, response_time, token counts\n",
    "    \"\"\"\n",
    "    start_time = time.time()  # Start timer for response time calculation\n",
    "    result = None\n",
    "    error_occurred = False\n",
    "    \n",
    "    try:\n",
    "        logger.info(f\"üîç Processing query: {question[:50]}...\")\n",
    "        \n",
    "        # Wrap advisor.query() with automatic retry on API errors\n",
    "        # @retry_on_api_error automatically retries up to 2 times on failures\n",
    "        @retry_on_api_error(max_retries=2, delay=2)\n",
    "        def query_with_retry():\n",
    "            return advisor.query(question)\n",
    "        \n",
    "        # Call the wrapped query method\n",
    "        result = query_with_retry()\n",
    "        \n",
    "        # Calculate response time (elapsed time since start)\n",
    "        response_time = time.time() - start_time\n",
    "        result['response_time'] = response_time\n",
    "        \n",
    "        # Track token usage and cost if enabled\n",
    "        if track_cost:\n",
    "            # Estimate token usage based on input/output text length\n",
    "            # cost_tracker uses tiktoken to count tokens accurately\n",
    "            input_text = question\n",
    "            output_text = result['answer']\n",
    "            input_tokens, output_tokens = cost_tracker.add_tokens(input_text, output_text)\n",
    "            \n",
    "            # Store token counts in result for display/analysis\n",
    "            result['input_tokens'] = input_tokens\n",
    "            result['output_tokens'] = output_tokens\n",
    "        \n",
    "        # Log successful query completion with key metrics\n",
    "        mode = result['mode']\n",
    "        logger.info(f\"‚úÖ Query completed in {response_time:.2f}s | Mode: {mode} | Relevance: {result['relevance_score']:.3f}\")\n",
    "        \n",
    "        # Track this query in experiment tracker for statistical analysis\n",
    "        # This accumulates data for comparing different hyperparameter settings\n",
    "        if log_to_experiment and hasattr(experiment_tracker, 'current_experiment') and experiment_tracker.current_experiment:\n",
    "            experiment_tracker.log_query(response_time, mode)\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Error handling: Catch any exceptions and return error result\n",
    "        error_occurred = True\n",
    "        response_time = time.time() - start_time\n",
    "        \n",
    "        logger.error(f\"‚ùå Query failed after {response_time:.2f}s: {type(e).__name__} - {str(e)}\")\n",
    "        \n",
    "        # Return error result with user-friendly message\n",
    "        return {\n",
    "            'mode': 'ERROR',\n",
    "            'answer': f\"Sorry, an error occurred while processing your question: {str(e)}\",\n",
    "            'relevance_score': 0.0,\n",
    "            'response_time': response_time,\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "# Create a convenience function that uses the global advisor\n",
    "def ask(question):\n",
    "    \"\"\"\n",
    "    Convenient shortcut function to ask a question with automatic monitoring.\n",
    "    \n",
    "    This is a simpler interface - just call ask(question) instead of \n",
    "    monitored_query(advisor, question).\n",
    "    \n",
    "    Usage:\n",
    "        result = ask(\"What are the key degradation mechanisms of a blue phosphorescent OLED?\")\n",
    "        print(result['answer'])\n",
    "    \n",
    "    Args:\n",
    "        question: User's question text\n",
    "    \n",
    "    Returns:\n",
    "        dict: Result dictionary with answer and metadata\n",
    "    \"\"\"\n",
    "    return monitored_query(advisor, question)\n",
    "\n",
    "print(\"‚úÖ Monitored query function ready!\")\n",
    "print(\"üí° Use ask(question) for quick queries with automatic monitoring\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Strict RAG System\n",
    "\n",
    "Test the advisor with sample questions to see how RAG, NO_ANSWER, and OFF_TOPIC modes are selected.\n",
    "\n",
    "---\n",
    "### Quick Reference: The Score System\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  RELEVANCE SCORE (0.0 - 1.0)                                    ‚îÇ\n",
    "‚îÇ  Question: \"Does the document match the query?\"                 ‚îÇ\n",
    "‚îÇ  ‚Ä¢ 0.85 -> Documents highly relevant to query ‚úì                 ‚îÇ\n",
    "‚îÇ  ‚Ä¢ 0.54 -> Documents somewhat matching (uncertain zone)         ‚îÇ\n",
    "‚îÇ  ‚Ä¢ 0.30 -> Documents not relevant to query ‚úó                    ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  RAG SCORE (1-10)                                               ‚îÇ\n",
    "‚îÇ  Question: \"How good is the RAG answer?\"                        ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Measures: Specificity + Relevance + Factuality               ‚îÇ\n",
    "‚îÇ  ‚Ä¢ 8.5 -> High-quality answer with specific facts from docs     ‚îÇ\n",
    "‚îÇ  ‚Ä¢ 6.0 -> Acceptable answer but lacking details                 ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "KEY INSIGHTS:\n",
    "   1. Low Relevance = No OLED-related documents -> OFF_TOPIC rejection\n",
    "   2. High Relevance + RAG \"no info\" = Relevant docs but no specific answer -> NO_ANSWER\n",
    "   3. High Relevance + Answer success = Accurate document-based answer -> RAG mode\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 18: Run Advisor Core Logic with Test Questions\n",
    "# Execute advisor with predefined test questions and format the results for display\n",
    "\n",
    "# HTML formatting function - for better result display\n",
    "def show_response(result, question):\n",
    "    \"\"\"\n",
    "    Display Advisor response in a formatted HTML box.\n",
    "    Adapted for Strict RAG mode (Project 2).\n",
    "    \"\"\"\n",
    "    mode = result['mode']\n",
    "    answer = result['answer']\n",
    "    relevance_score = result['relevance_score']\n",
    "    \n",
    "    # Color coding by mode (Strict RAG: RAG, NO_ANSWER_IN_DOCS, OFF_TOPIC only)\n",
    "    if mode == \"RAG\":\n",
    "        color = \"#4caf50\"  # Green\n",
    "        icon = \"üü¢\"\n",
    "        mode_text = \"RAG Mode (Document-based Response)\"\n",
    "    elif mode == \"NO_ANSWER_IN_DOCS\":\n",
    "        color = \"#ff9800\"  # Orange\n",
    "        icon = \"üü†\"\n",
    "        mode_text = \"No Answer (No relevant content in documents)\"\n",
    "    elif mode == \"OFF_TOPIC\":\n",
    "        color = \"#f44336\"  # Red\n",
    "        icon = \"üî¥\"\n",
    "        mode_text = \"Off-Topic Rejection (Low Relevance, Auto-rejected)\"\n",
    "    elif mode == \"ERROR\":\n",
    "        color = \"#f44336\"  # Red\n",
    "        icon = \"‚ùå\"\n",
    "        mode_text = \"System Error\"\n",
    "    else:\n",
    "        color = \"#9e9e9e\"  # Gray\n",
    "        icon = \"‚ö™\"\n",
    "        mode_text = f\"Unknown Mode: {mode}\"\n",
    "    \n",
    "    # Create metadata line (including relevance score)\n",
    "    metadata = f\"{icon} <strong>{mode_text}</strong> | Relevance Score: {relevance_score:.3f}\"\n",
    "    \n",
    "    html = f\"\"\"\n",
    "    <div style=\"\n",
    "        background-color:#f9f9f9;\n",
    "        border-left: 6px solid {color};\n",
    "        padding: 15px;\n",
    "        margin: 15px 0;\n",
    "        border-radius: 4px;\n",
    "        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Arial, sans-serif;\">\n",
    "        <div style=\"color:#333; font-size:14px; margin-bottom:10px;\">\n",
    "            <strong>Question:</strong> {question}\n",
    "        </div>\n",
    "        <div style=\"color:#666; font-size:13px; margin-bottom:10px;\">\n",
    "            {metadata}\n",
    "        </div>\n",
    "        <div style=\"color:#111; font-size:14px; line-height:1.6; white-space: pre-wrap;\">\n",
    "            <strong>Answer:</strong><br>{answer}\n",
    "        </div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    display(HTML(html))\n",
    "\n",
    "# Run advisor core logic with test questions\n",
    "print(\"Running advisor core logic with test questions...\\n\")\n",
    "\n",
    "# Start experiment tracking\n",
    "experiment_tracker.start_experiment({\n",
    "    # RAG Quality (Step 1)\n",
    "    'chunk_size': CHUNK_SIZE,\n",
    "    'chunk_overlap': CHUNK_OVERLAP,\n",
    "    'top_k': TOP_K_DOCUMENTS,\n",
    "    'temperature': LLM_TEMPERATURE,\n",
    "    \n",
    "    # System Architecture (Step 2) - Used for Strict RAG only\n",
    "    'relevance_threshold': RELEVANCE_THRESHOLD,\n",
    "    'sigmoid_midpoint': SIGMOID_MIDPOINT,\n",
    "    'sigmoid_steepness': SIGMOID_STEEPNESS\n",
    "})\n",
    "\n",
    "# Reset cost tracker for this experiment\n",
    "cost_tracker.reset()\n",
    "\n",
    "# Store relevance scores for later averaging\n",
    "relevance_scores = []\n",
    "\n",
    "for i, question in enumerate(TEST_QUERIES, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Test {i}/{len(TEST_QUERIES)}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Use monitored_query() instead of advisor.query()\n",
    "    result = monitored_query(advisor, question)\n",
    "    show_response(result, question)\n",
    "    \n",
    "    # Store relevance score\n",
    "    if 'relevance_score' in result:\n",
    "        relevance_scores.append(result['relevance_score'])\n",
    "    \n",
    "    # Display performance metrics\n",
    "    if 'response_time' in result:\n",
    "        print(f\"Response Time: {result['response_time']:.2f}s\")\n",
    "    if 'input_tokens' in result and 'output_tokens' in result:\n",
    "        print(f\"Tokens: {result['input_tokens']} input + {result['output_tokens']} output = {result['input_tokens'] + result['output_tokens']} total\")\n",
    "\n",
    "# Print experiment summary\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"Experiment Summary\")\n",
    "print(f\"{'='*80}\")\n",
    "cost_summary = cost_tracker.get_summary()\n",
    "print(f\"Total Tokens: {cost_summary['total_tokens']:,}\")\n",
    "print(f\"Estimated Cost: ${cost_summary['estimated_cost_usd']:.4f}\")\n",
    "print(f\"Mode Distribution:\")\n",
    "for mode, count in experiment_tracker.current_experiment['mode_counts'].items():\n",
    "    if count > 0:\n",
    "        print(f\"   - {mode}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantitative Evaluation: System Quality\n",
    "\n",
    "**What This Section Does:**\n",
    "Evaluates the quality of answers generated by the Strict RAG system.\n",
    "\n",
    "**Evaluation Process:**\n",
    "1. **For each test question:**\n",
    "   - Generate answer using the Strict RAG Advisor\n",
    "   - If an answer is provided (not rejected), evaluate with LLM-as-a-judge\n",
    "\n",
    "2. **Evaluation Metrics** (1-10 scale):\n",
    "   - **Specificity**: How detailed is the answer?\n",
    "   - **Relevance**: Does it directly answer the question?\n",
    "   - **Factuality**: Does it contain verifiable facts from the documents?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 20: Quantitative Evaluation - System Quality Check\n",
    "# This cell implements the RAGEvaluator class for evaluating system performance.\n",
    "# Note: LLM baseline comparison has been removed to focus on Strict RAG quality.\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "class RAGEvaluator:\n",
    "    \"\"\"Evaluate the final advisor answer quality.\"\"\"\n",
    "    \n",
    "    def __init__(self, llm, advisor):\n",
    "        self.llm = llm\n",
    "        self.advisor = advisor\n",
    "    \n",
    "    def get_final_system_answer(self, question):\n",
    "        \"\"\"Get the final answer from the full advisor system.\"\"\"\n",
    "        # track_cost=False, log_to_experiment=False to avoid double counting\n",
    "        result = monitored_query(self.advisor, question, track_cost=False, log_to_experiment=False)\n",
    "        return result\n",
    "    \n",
    "    def score_answer(self, question, answer):\n",
    "        \"\"\"Score an answer on specificity, relevance, and factuality (1-10).\"\"\"\n",
    "        evaluation_prompt = f\"\"\"You are an expert evaluator. Score the following answer on these three criteria (scale 1-10):\n",
    "\n",
    "1. SPECIFICITY: How specific and detailed is the answer? (1 = vague, 10 = very specific with details)\n",
    "2. RELEVANCE: How relevant is the answer to the question? (1 = off-topic, 10 = directly answers question)\n",
    "3. FACTUALITY: Does the answer contain verifiable facts and data? (1 = no facts/opinions only, 10 = rich with facts and data)\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: {answer}\n",
    "\n",
    "Respond ONLY with a JSON object in this exact format (no other text):\n",
    "{{\"specificity\": <score>, \"relevance\": <score>, \"factuality\": <score>}}\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.llm.invoke(evaluation_prompt).content\n",
    "            start_idx = response.find('{')\n",
    "            end_idx = response.rfind('}') + 1\n",
    "            json_str = response[start_idx:end_idx]\n",
    "            scores = json.loads(json_str)\n",
    "            return scores\n",
    "        except:\n",
    "            return {\"specificity\": 5, \"relevance\": 5, \"factuality\": 5}\n",
    "    \n",
    "    def evaluate_system(self, question):\n",
    "        \"\"\"Evaluate final system answer quality.\"\"\"\n",
    "        result = self.get_final_system_answer(question)\n",
    "        final_answer = result.get(\"answer\", \"\")\n",
    "        mode = result.get(\"mode\", \"UNKNOWN\")\n",
    "        \n",
    "        # If no answer (rejected), we can't score specificity/factuality nicely\n",
    "        # But we can track that it was rejected.\n",
    "        if mode in [\"OFF_TOPIC\", \"NO_ANSWER_IN_DOCS\"]:\n",
    "             return {\n",
    "                \"question\": question,\n",
    "                \"final_answer\": final_answer,\n",
    "                \"mode\": mode,\n",
    "                \"final_specificity\": 0,\n",
    "                \"final_relevance\": 0,\n",
    "                \"final_factuality\": 0,\n",
    "                \"final_word_count\": 0\n",
    "            }\n",
    "            \n",
    "        # Score the answer\n",
    "        final_scores = self.score_answer(question, final_answer)\n",
    "        final_word_count = len(final_answer.split())\n",
    "        \n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"final_answer\": final_answer,\n",
    "            \"mode\": mode,\n",
    "            \"final_specificity\": final_scores[\"specificity\"],\n",
    "            \"final_relevance\": final_scores[\"relevance\"],\n",
    "            \"final_factuality\": final_scores[\"factuality\"],\n",
    "            \"final_word_count\": final_word_count,\n",
    "        }\n",
    "\n",
    "# Initialize evaluator\n",
    "evaluator = RAGEvaluator(advisor.llm, advisor)\n",
    "\n",
    "# Run evaluation\n",
    "print(\"üìä Running system quality evaluation...\\n\")\n",
    "evaluation_results = []\n",
    "\n",
    "for i, question in enumerate(TEST_QUERIES, 1):\n",
    "    print(f\"Evaluating {i}/{len(TEST_QUERIES)}: {question[:50]}...\")\n",
    "    result = evaluator.evaluate_system(question)\n",
    "    evaluation_results.append(result)\n",
    "    print(\"  ‚úì Complete\")\n",
    "\n",
    "print(\"\\n‚úÖ Evaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Results Analysis\n",
    "This section analyzes the performance of the Strict RAG system, calculating quality scores for answered questions and tracking rejection counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 22: System Evaluation Analysis\n",
    "# This cell displays performance metrics for the Strict RAG system.\n",
    "\n",
    "# Convert results to DataFrame\n",
    "df = pd.DataFrame(evaluation_results)\n",
    "\n",
    "# Filter only answered questions for scoring stats (exclude rejections)\n",
    "answered_df = df[~df['mode'].isin(['OFF_TOPIC', 'NO_ANSWER_IN_DOCS'])]\n",
    "\n",
    "if not answered_df.empty:\n",
    "    avg_specificity = answered_df['final_specificity'].mean()\n",
    "    avg_relevance = answered_df['final_relevance'].mean()\n",
    "    avg_factuality = answered_df['final_factuality'].mean()\n",
    "    avg_words = answered_df['final_word_count'].mean()\n",
    "    \n",
    "    # Calculate overall score\n",
    "    answered_df['final_overall'] = (answered_df['final_specificity'] + answered_df['final_relevance'] + answered_df['final_factuality']) / 3\n",
    "    avg_overall = answered_df['final_overall'].mean()\n",
    "else:\n",
    "    avg_specificity = 0\n",
    "    avg_relevance = 0\n",
    "    avg_factuality = 0\n",
    "    avg_words = 0\n",
    "    avg_overall = 0\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"=\" * 80)\n",
    "print(\"üìä STRICT RAG SYSTEM PERFORMANCE SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total Questions: {len(df)}\")\n",
    "print(f\"Answered: {len(answered_df)}\")\n",
    "print(f\"Rejected/No Data: {len(df) - len(answered_df)}\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Metric (Answered Queries Only)':<35} {'Score':<15}\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Specificity (1-10)':<35} {avg_specificity:<15.2f}\")\n",
    "print(f\"{'Relevance (1-10)':<35} {avg_relevance:<15.2f}\")\n",
    "print(f\"{'Factuality (1-10)':<35} {avg_factuality:<15.2f}\")\n",
    "print(f\"{'Overall Score (1-10)':<35} {avg_overall:<15.2f}\")\n",
    "print(f\"{'Average Word Count':<35} {avg_words:<15.1f}\")\n",
    "print()\n",
    "\n",
    "# Display detailed results table\n",
    "print(\"=\" * 80)\n",
    "print(\"üìã DETAILED RESULTS BY QUESTION\")\n",
    "print(\"=\" * 80)\n",
    "display(df[['question', 'mode', 'final_answer']])\n",
    "\n",
    "# Calculate average relevance score from Cell 18's data\n",
    "# relevance_scores list is collected from Cell 18\n",
    "if 'relevance_scores' in dir() and len(relevance_scores) > 0:\n",
    "    avg_relevance_score = sum(relevance_scores) / len(relevance_scores)\n",
    "else:\n",
    "    avg_relevance_score = None\n",
    "    print(\"‚ö†Ô∏è Warning: relevance_scores not found. Run Cell 18 first.\")\n",
    "\n",
    "# Calculate Q1-Q2 vs Q5-Q6 separation (for CSV logging)\n",
    "relevance_mean_pos = None\n",
    "relevance_mean_neg = None\n",
    "relevance_gap = None\n",
    "relevance_margin = None\n",
    "\n",
    "try:\n",
    "    if 'relevance_scores' in dir() and len(relevance_scores) >= 6:\n",
    "        # Q1, Q2 -> OLED core questions (Positive)\n",
    "        pos_indices = [0, 1]\n",
    "        # Q5, Q6 -> Completely OFF-TOPIC questions (Negative)\n",
    "        neg_indices = [4, 5]\n",
    "\n",
    "        pos_scores = [float(relevance_scores[i]) for i in pos_indices]\n",
    "        neg_scores = [float(relevance_scores[i]) for i in neg_indices]\n",
    "\n",
    "        relevance_mean_pos = sum(pos_scores) / len(pos_scores)\n",
    "        relevance_mean_neg = sum(neg_scores) / len(neg_scores)\n",
    "        relevance_gap = relevance_mean_pos - relevance_mean_neg\n",
    "        relevance_margin = min(pos_scores) - max(neg_scores)\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Need at least 6 relevance scores (Q1‚ÄìQ6) to compute separation metrics.\")\n",
    "except Exception as e:\n",
    "    print(\"‚ö†Ô∏è Failed to compute relevance separation metrics:\", e)\n",
    "\n",
    "# Save experiment results\n",
    "if hasattr(experiment_tracker, 'save_experiment'):\n",
    "    # notes: Save brief summary text (also record LLM settings)\n",
    "    notes_str = (\n",
    "        f\"model={LLM_MODEL}, provider={LLM_PROVIDER}, \"\n",
    "        f\"Strict RAG Evaluation: {len(answered_df)}/{len(df)} answered.\"\n",
    "    )\n",
    "\n",
    "    experiment_tracker.save_experiment(\n",
    "        avg_relevance_score=avg_relevance_score,\n",
    "        avg_rag_score=avg_overall,\n",
    "        avg_rag_specificity=avg_specificity,\n",
    "        avg_rag_relevance=avg_relevance,\n",
    "        avg_rag_factuality=avg_factuality,\n",
    "        rel_mean_pos=relevance_mean_pos,\n",
    "        rel_mean_neg=relevance_mean_neg,\n",
    "        rel_gap=relevance_gap,\n",
    "        rel_margin=relevance_margin,\n",
    "        notes=notes_str,\n",
    "    )\n",
    "\n",
    "    # Console output always uses the same format\n",
    "    mean_pos_str = \"N/A\" if relevance_mean_pos is None else f\"{relevance_mean_pos:.3f}\"\n",
    "    mean_neg_str = \"N/A\" if relevance_mean_neg is None else f\"{relevance_mean_neg:.3f}\"\n",
    "    gap_str = \"N/A\" if relevance_gap is None else f\"{relevance_gap:.3f}\"\n",
    "    margin_str = \"N/A\" if relevance_margin is None else f\"{relevance_margin:.3f}\"\n",
    "\n",
    "    print(f\"\\nüìä Average Relevance Score: {avg_relevance_score:.3f}\" if avg_relevance_score else \"\")\n",
    "    print(\n",
    "        f\"üìä Relevance separation (Q1‚ÄìQ2 vs Q5‚ÄìQ6): \"\n",
    "        f\"mean_pos={mean_pos_str}, \"\n",
    "        f\"mean_neg={mean_neg_str}, \"\n",
    "        f\"gap={gap_str}, margin={margin_str}\"\n",
    "    )\n",
    "    print(\"‚úÖ Experiment results saved to CSV.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Q&A Interface\n",
    "Now you can ask the AI-Driven OLED Assistant your own questions directly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 26: Interactive Q&A Interface\n",
    "# Note: This cell handles duplicate handler registration when run multiple times\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import clear_output, display, HTML\n",
    "\n",
    "# Global flag to prevent duplicate registration\n",
    "if 'OLED_UI_INITIALIZED' not in dir():\n",
    "    OLED_UI_INITIALIZED = False\n",
    "\n",
    "# Remove existing UI if present\n",
    "if OLED_UI_INITIALIZED:\n",
    "    try:\n",
    "        oled_ui_box.close()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Create widgets\n",
    "oled_question_input = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder='Enter your OLED Display related question...',\n",
    "    description='Question:',\n",
    "    layout=widgets.Layout(width='90%', height='80px'),\n",
    "    style={'description_width': '50px'}\n",
    ")\n",
    "\n",
    "oled_submit_btn = widgets.Button(\n",
    "    description='Get Answer',\n",
    "    button_style='primary',\n",
    "    layout=widgets.Layout(width='150px')\n",
    ")\n",
    "\n",
    "oled_output = widgets.Output()\n",
    "\n",
    "def oled_handle_submit(btn):\n",
    "    with oled_output:\n",
    "        clear_output(wait=True)\n",
    "        question = oled_question_input.value.strip()\n",
    "        if not question:\n",
    "            print(\"Please enter a question!\")\n",
    "            return\n",
    "        \n",
    "        print(\"Analyzing question...\")\n",
    "        result = advisor.query(question)\n",
    "        \n",
    "        mode = result['mode']\n",
    "        answer = result['answer']\n",
    "        score = result['relevance_score']\n",
    "        \n",
    "        if mode == \"RAG\":\n",
    "            color, icon, text = \"#4caf50\", \"üü¢\", \"RAG Mode (Document-based)\"\n",
    "        elif mode == \"NO_ANSWER_IN_DOCS\":\n",
    "            color, icon, text = \"#ff9800\", \"üü†\", \"No Answer (Not in documents)\"\n",
    "        else:\n",
    "            color, icon, text = \"#f44336\", \"üî¥\", \"Off-Topic (Rejected)\"\n",
    "        \n",
    "        html = f'''<div style=\"background:#f5f5f5; border-left:5px solid {color}; padding:12px; margin:8px 0; border-radius:4px;\">\n",
    "<b>Q:</b> {question}<br>\n",
    "<span style=\"color:#666\">{icon} {text} | Relevance: {score:.3f}</span><br><br>\n",
    "<b>A:</b> {answer}\n",
    "</div>'''\n",
    "        display(HTML(html))\n",
    "        \n",
    "        if mode == 'RAG' and result.get('retrieved_docs'):\n",
    "            print(\"\\nRetrieved documents:\")\n",
    "            for i, doc in enumerate(result['retrieved_docs'][:2], 1):\n",
    "                print(f\"  {i}. {doc.page_content[:120]}...\")\n",
    "\n",
    "oled_submit_btn.on_click(oled_handle_submit)\n",
    "\n",
    "# Bundle into UI box\n",
    "oled_ui_box = widgets.VBox([\n",
    "    widgets.HTML(\"<h3>üí¨ AI-Driven OLED Assistant</h3>\"),\n",
    "    oled_question_input,\n",
    "    oled_submit_btn,\n",
    "    oled_output\n",
    "])\n",
    "\n",
    "OLED_UI_INITIALIZED = True\n",
    "display(oled_ui_box)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
